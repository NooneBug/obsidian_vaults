Described by [[2018 Ramponi - Neural Unsupervised Domain Adaptation in NLP, A Survey]]: 

*First, the Penn Treebank WSJ corpus (Marcus et al., 1993) and the Brown corpus (Francis and Kucera, 1979) are prototypical examples, with the WSJ being considered widely as the canonical newswire domain. In the recent decade, there has been considerable work on what is considered non-canonical data. The dichotomy between canonical (typically considered well-edited English newswire) and noncanonical data arose with the increasing interest of working with social media with all its challenges related to the ‘noisiness’ of the domain (Eisenstein, 2013; Baldwin et al., 2013). Models trained on canonical data failed in light of the challenges on, e.g., Twitter (Gimpel et al., 2011; Foster et al., 2011).*

So a canonical corpus typically is a well-edited English newswire, a non-canonical corpus is a corpus which comes from social media and or is non-well edited, defined also in [[Canonical Language Definition]]

But [[2018 Ramponi - Neural Unsupervised Domain Adaptation in NLP, A Survey]] also reports a criticity: 

*However, what is a domain? Is, say, Twitter, its own domain? Or is it a set of subdomains? Similarly, do language samples of social groups (e.g., sociolects) form a domain or a set of subdomains?*

This definition is contrasted by [[2016 Plank - What to do about non-standard (or non-canonical) language in NLP]]