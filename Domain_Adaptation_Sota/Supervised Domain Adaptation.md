From [[2018 Ramponi - Neural Unsupervised Domain Adaptation in NLP, A Survey]]:

Work on domain adaptation focused largely on supervised domain adaptation [[2009 Daume - Frustratingly Easy Domain Adaptation]][[2011 Plank - Domain Adaptation for Parsing]]. In such a classic supervised DA setup, a small amount of labeled target domain data is available, along with some larger amount of labeled source domain data. The task is to adapt from the source to the specific target domain in light of limited target domain data. However, annotation is a substantial timerequiring and costly manual effort. While annotation directly mitigates the lack of labeled data, it does not easily scale to new application targets. In contrast, DA methods aim to shift the ability of models from the traditional interpolation of similar examples to models that extrapolate to examples outside the original training distribution [[2019 Ruder - Neural Transfer Learning for Natural Language Processing]]. 