For many target applications, labeled data is lacking (Y scarcity), and even for pre-training general models data might be scarce (X scarcity). This makes it even more pressing to revisit a particular type of transfer learning, namely domain adaptation (DA) [[2018 Ramponi - Neural Unsupervised Domain Adaptation in NLP, A Survey]]

Formally, training and test data are assumed to be independently and identically (i.i.d.) sampled from the same underlying distribution. In practice, this assumption does not hold, which translates into a drop in performance when the model f trained on a source domain S is tested on a different but related target domain T. [[2018 Ramponi - Neural Unsupervised Domain Adaptation in NLP, A Survey]]

[[2021 Hedderic - A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios]] categorize low-resource scenarios in three categories [[Low-resources scenarios categories]], the problems discussed in this (Vimercati et al) paper regards two of this three categories, namely:  (scarce) availability of task-specific labels, and availability of auxiliary data

Most research in machine learning, both theoretical and empirical, assumes that models are trained and tested using data drawn from some fixed distribution. This single domain setting has been well studied, and uniform convergence theory guarantees that a modelâ€™s empirical training error is close to its true error under such assumptions. In many practical cases, however, we wish to train a model in one or more source domains and then apply it to a different target domain. [[2010 Ben David - A theory of learning from different domains]]