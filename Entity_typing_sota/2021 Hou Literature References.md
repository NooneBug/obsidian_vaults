Survey-like information from [[2021 Hou - Transfer learning for fine-grained entity typing]]:

Denoising approaches: 
---

"The semantic tags in the knowledge base are then mapped to the type taxonomy. This inevitably induces label noises in the training data. There are mainly two approaches to address this issue. One approach is to use heuristics to preprocess the training data. Such heuristics usually remove less frequent types in a document or more fine grained types. As a result, the preprocessing makes the training corpus skewed toward coarse grained type labels [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]]. The other approach is to conservatively discourage the typing model from predicting more fine-grained types [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]]."

Encoders:
---

"The typing model usually makes predictions on the features extracted from the context and the words that consist of the entity mention. The features used by traditional machine learning algorithms are binary feature functions [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names]],[[2010 Rahman - Inducing Fine-Grained Semantic Classes via Hierarchical and Collective Classification]], while the features used by deep learning methods are embedded dense dimensional vectors generated by recurrent neural networks [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]] [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification]], such as long short-term memory (LSTM). The former suffers from feature sparsity, while the latter suffers from insufficient feature representations. These features are extracted from the local context of a small window, while the document-level features are seldom used."

"The feature representations have significant influence on the performance of classifiers. The features used by traditional machine learning methods [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]] [[2012 Ling - Fine-Grained Entity Recognition]] are hand-crafted binary feature functions. To combat the feature sparsity, some methods [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]] [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing]] embed these binary features into low-dimensional vectors. The neural network models for FGET usually employ different neural networks (with different parameters) to embed the mention features and context features. Most of the methods [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings]] [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]] [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification]] use LSTM to encode the mentions and contexts"


Topic Models:
---

We show that the topic model is capable of transferring the learned associations between semantic types and hidden topics. The document level topic label has been used as a feature for FGET [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]], [[2016 Suzuki - Fine-grained named entity classification with wikipedia article vectors]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]] or used to reduce label noises  [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]].

Mention level ET and Entity Level ET:
---

There are two categories of FGET: The mention-level FGET [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]] and the entity-level FGET  [[2018 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing]]. The former is to determine the semantic type of an entity mention in a particular context, while the latter is to find all the possible semantic types of an entity. For example, Donald J. Trump can be a /Person/political_figure or /Person/business in different contexts; thus, the set of entity-level types of Donald J. Trump includes the aforementioned two types. Our research falls under the mention-level FGET.

Typing Architecture: 
---

FGET is typically a multi-class multi-label classification problem. It is also a hierarchical classification problem because of the hierarchical relations among the type labels. According to the categorization of hierarchical classifiers by, there are mainly four categories of classifiers used in FGET: 
- Flat: Using a single multi-class classifier for all types. Such classifiers mainly include: decision tree [[2002 Fleischman - Fine Grained Classification of Named Entities]], linear classifier [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings]], [[2012 Ling - Fine-Grained Entity Recognition]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]] [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing]], and softmax classifier [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]], [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions]], 
- Local: Using a binary classifier for each type, enforcing label consistency at inference time. Such classifiers include: SVM [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names]], maximum entropy classifier or logistic regression classifier [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]] [[2016 Suzuki - Fine-grained named entity classification with wikipedia article vectors]]. 
- Local per Parent Node: Using multi-class classifier for all children of a parent type node. The RNN encoderâ€“decoder-based typing model of falls into this category. 
- Global: Using a single multi-class classifier trained with loss function that considers label similarity and hierarchical relationships. The typing models of [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]], [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments]] fall into this category.

Noise managing
---
"To tackle the label noises, previous research either directly tweaks the noisy labels using heuristics, such as [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]], [[2018 Choi - Ultra-Fine Entity Typing]] [[2018 - Transforming Wikipedia into a large-scale fine-grained entity type corpus.]], or enhances the typing model with the ability of tackling label noises, such as [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings]], [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]][[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding]][[2016 Ren  - Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding]]. The former approach makes the training corpus skewed toward coarse-grained labels, while the latter approach encourages the typing model to predict relevant parent-types. Thus both approaches tend to retreat to coarse-grained types. We train our typing classifier on intact noisy corpus."