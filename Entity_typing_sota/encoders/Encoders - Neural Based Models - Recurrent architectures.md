Models which use an recurrent architectures to encode the entity mention within its sentence.

Architectures can be:

RNN/Bi-RNN:
[[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions]]

LSTM/Bi-LSTM:
[[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification]]
[[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings]]
[[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification]]
[[2018 Choi - Ultra-Fine Entity Typing]]
[[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention]]
[[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]]
[[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds]]
[[2018 Yuan - OTyper , A Neural Architecture for Open Named Entity Typing]]
[[2019 Chen - Improving Distantly-supervised Entity Typing with Compact Latent Space Clustering]]
[[2019 Wu - Modeling Noisy Hierarchical Types in Fine-Grained Entity Typing, A Content-Based Weighting Approach]]
[[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing]]
[[2019 Onoe - Learning to Denoise Distantly-Labeled Data for Entity Typing]]
[[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing]]
[[2019 Sahay - A Type-Specific Attention Model For Fine Grained Entity Type Classification]]
[[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking]]
[[2021 Hou - Transfer learning for fine-grained entity typing]]

All models with [[Encoders - Neural Based Models - ELMo based architectures]]

#encoders