Models which use an recurrent architectures to encode the entity mention within its sentence.

Architectures can be:

RNN:
[[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions]]

LSTM/Bi-LSTM:
[[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification]]
[[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings]]
[[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification]]
[[2018 Choi - Ultra-Fine Entity Typing]]
[[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention]]
[[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]]

#encoders