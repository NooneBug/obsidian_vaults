The ultra-fine entity typing task proposed by [[2018 Choi - Ultra-Fine Entity Typing | Choi et al. (2018)]] uses a large, open type vocabulary to achieve better type coverage than the traditional fine-grained entity typing task [[2012 Ling - Fine-Grained Entity Recognition|(Ling and Weld, 2012)]] that uses manually designed entity type ontologies. There are only limited studies on this newly proposed task: A neural model introduced by [[2019 Onoe - Learning to Denoise Distantly-Labeled Data for Entity Typing | (Onoe and Durrett, 2019)]] filters samples that are too noisy to be used and relabels the remaining samples to get cleaner labels. A graph propagation layer is introduced by [[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing | (Xiong et al., 2019)]] to impose a label-relational bias on entity typing models, so as to implicitly capture type dependencies. [[2021 Onoe - Modeling Fine-Grained Entity Types with Box Embeddings | Onoe et al. (2021)]] use box embeddings to capture latent type hierarchies. There is also some work on the applications of ultra-fine entity typing: Onoe and Durrett (2020)

apply ultra-fine entity typing to learn entity representations for two downstream tasks: coreference arc prediction and named entity disambiguation. The traditional fine-grained entity typing task ([[2012 Ling - Fine-Grained Entity Recognition | Ling and Weld, 2012]]; [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names | Yosef et al., 2012]]) is closely related to ultra-fine entity typing. Automatic annotation ([[2012 Ling - Fine-Grained Entity Recognition | Ling and Weld, 2012]]; [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging | Gillick et al., 2014]]; [[2020 Dai - Exploiting Semantic Relations for Fine-grained Entity Typing | Dai et al., 2020]]) is also commonly used in the studies of this task to produce large size training data. Many different approaches have been proposed to improve fine-grained entity typing performance. For example, denoising the automatically generated labels [[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding | (Ren et al., 2016)]], taking advantage of the entity type hierarchies or type inter-dependencies [[2020 Chen - Hierarchical Entity Typing via Multi-level Learning to Rank | (Chen et al., 2020]]; [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking | Murty et al., 2018]]; [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation | Lin and Ji, 2019]]), exploiting external resources such as the information of entities provided in knowledge bases ([[2019 Jin - Fine-grained entity typing via hierarchical multi graph convolutional networks | Jin et al., 2019]]; [[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking | Dai et al., 2019]]; [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention | Xin et al., 2018]]), etc.