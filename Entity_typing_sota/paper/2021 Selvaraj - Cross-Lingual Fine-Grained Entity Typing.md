https://arxiv.org/pdf/2110.07837.pdf

#### Abstract
The growth of cross-lingual pre-trained models has enabled NLP tools to rapidly generalize to new languages. While these models have been applied to tasks involving entities, their ability to explicitly predict typological features of these entities across languages has not been established. In this paper, we present a unified cross-lingual fine-grained entity typing model capable of handling over 100 languages and analyze this model’s ability to generalize to languages and entities unseen during training. We train this model on cross-lingual training data collected from Wikipedia hyperlinks in multiple languages (training languages). During inference, our model takes an entity mention and context in a particular language (test language, possibly not in the training languages) and predicts fine-grained types for that entity. Generalizing to new languages and unseen entities are the fundamental challenges of this entity typing setup, so we focus our evaluation on these settings and compare against simple yet powerful string match baselines. Experimental results show that our approach outperforms the baselines on unseen languages such as Japanese, Tamil, Arabic, Serbian, and Persian. In addition, our approach substantially improves performance on unseen entities (even in unseen languages) over the baselines, and human evaluation shows a strong ability to predict relevant types in these settings.

#### Introduction

Most prior work in fine-grained entity typing has predominantly focused on monolingual models in English (Yaghoobzadeh and Schütze, 2015; Shimaoka et al., 2017; Dai et al., 2019) or other often high-resource languages (van Erp and Vossen, 2017; Lee et al., 2020). Unified cross-lingual entity typing models that can cover a wide range of languages have never been established. However, the cross-lingual setting is vital for the accessibility, inclusivity, and success of future NLP systems, to better serve more of the world’s population (Joshi et al., 2020).

In this work, we first define a cross-lingual entity typing task that associates entity mentions in any language with predefined fine-grained types without employing translation. Then, we build a unified cross-lingual entity typing model that can take an entity mention in context in over 100 different languages, outputting fine-grained types for that entity. To train this model, we automatically create entity typing datasets based on distant supervision techniques first explored in Mintz et al. (2009), using Wikipedia articles in four languages: English, Finnish, German, and Spanish. Our typing model uses an encoder based on pre-trained multilingual BERT (Devlin et al., 2019, mBERT) to output 10k fine-grained types derived from the Wikipedia categories. Figure 1 shows an overview of our system.

![[2021_selvaraj_overview.png]]

Our evaluation primarily focuses on two questions: 
1) can a cross-lingual entity typing model generalize to unseen languages? 
2) how does a cross-lingual entity typing model handle unseen entities? 

For the first question, we design a zero-shot languages experimental setup in which an entity typing model is trained on the training languages (English, Finnish, German, and Spanish) and evaluated on test languages (such as Tamil) which are unseen during training. To investigate the second question, we perform experiments on unseen entities, where the entity typing model is evaluated a set of entities held out from the model during training. In our experiments, we use entity typing test examples adapted from Mewsli-9 (Botha et al., 2020), an entity linking dataset derived from WikiNews (Section 3.2). We compare our model with a string match baseline (Section 5.1), which is simple but shows strong performance on the seen entities in the training languages, as well as a mention-string similarity approach. In addition, we perform human evaluation on the predicted types by our model and investigate performance breakdown by type frequency (Section 5.4 and Section 5.5).

In the zero-shot languages setting, our approach outperforms baselines on unseen languages by a substantial margin. Surprisingly, our models can make reasonable predictions on languages with different (non-Latin) scripts from the training languages, such as Arabic, Persian, Japanese, and Tamil, which suggests that our model generalizes to unseen languages. In the unseen entities setting, our model also shows much higher performance compared to the baselines, and can even predict plausible types for unseen entities in unseen languages.

#### Cross-Lingual Entity Typing

Our model takes an input sequence consisting of a mention in its context. Because our model encoder is pre-trained mBERT (Devlin et al., 2019), the model can accept input from any language of the 104 languages in the mBERT vocabulary(mBERT uses 110k subword tokens while the English BERT model has a vocabulary size of 30k; this higher number particularly includes a range of unicode characters and sequences beyond Latin script)

Next, a multi-label classifier outputs predictions for the input entity mention’s types. The model can handle a large typeset with tens of thousands of types. We train the model using text in English, Finnish, German, and Spanish, and evaluate its performance on Arabic, German, English, Spanish, Persian, Japanese, Serbian, Tamil, and Turkish. We analyze the entity typing performance in the zero-shot languages and unseen entities settings (Section 5.2).

Our list of predefined types T is in English, but in general, the types can be in any language; they should represent language-agnostic semantic meaning (this assumption might be flawed in two ways. First, an English typeset may be English-centric in type coverage, such as having more types around regions and states in the United States compared to other countries. Second, certain concepts in other languages may not be easily expressible in English). By controlling the number of types or the types themselves, this entity-typing approach can generalize to different settings and knowledge bases. 

#### Data Collection

Entity typing requires lots of annotated data to train large models, but manually annotating entities in natural text with types can be expensive, timeconsuming, and tedious (Choi et al., 2018). This is particularly true for low-resource languages. To address this, we automatically create entity typing datasets of millions of examples from Wikipedia in several languages using distant supervision. Our data pipeline is scalable to data in new languages and automates compiling large quantities of data from Wikipedia, as seen in Figure 2.

![[2021_selvaraj_dataset_building.png]]

The training data for our model consists of data which we compiled from Wikipedia in four different languages: English, German, Finnish, and Spanish. We then evaluate our model using data we adapted from Mewsli-9 (Botha et al., 2020), a WikiNews entity linking dataset in nine different languages.


**3.1 Training Data**

To train our entity typing model, we need labeled examples of $(m, s, t^∗)$ triples of an entity mention, context, and gold types. We start with a dump of Wikipedia articles in four languages: English, Finnish, German, and Spanish, languages picked for their dissimilarity (among languages supported by the SLING API) and to increase entity coverage. We sample all languages in a balanced way, which in practice takes a larger fraction of data from rarer languages. Table 1 shows a detailed breakdown of our training examples. We use Wikipedia hyperlinks as the mention sequences $m$, situating each mention with up to 50 context words from the article on either side of the hyperlinked text. This forms the entire context sequence $s$

![[2021_selvaraj_training_dataset.png]]

Using the SLING natural language frame semantics parser (Ringgaard et al., 2017) we connect each hyperlink to the WikiData QID of its target entity (the destination of the hyperlink). QIDs are language-agnostic identifiers that label each entity in the WikiData knowledge base. For each QID, we use the MediaWiki Action API to find the corresponding English Wikipedia page, and we derive the entity’s gold types $t^∗$ from the Wikipedia categories of this page. Lastly, we filter out examples that did not fit at least one of our 10,000 predefined types T (defined in Section 3.3), leaving us with 8.9 million distantly-supervised entity typing training examples.

**3.2 Test Data** 
We derive test data from Mewsli-9 (Botha et al., 2020), a dataset of entity mentions extracted from WikiNews articles. These entities come linked to WikiData QIDs, so we again used our pipeline with the MediaWiki Action API to annotate the mentions $m$ with gold types $t^∗$ based on the categories of the corresponding English Wikipedia pages. This process gave us typing datasets in nine languages: Arabic, German, English, Persian, Japanese, Serbian, Spanish, Turkish, and Tamil.

**3.3 Types** 
Our types are derived based on post-processing English Wikipedia categories. Using a set of rules from prior work (Onoe and Durrett, 2020a,b), we map each Wikipedia category to one or more coarser types based on removing information. Specifically, we apply lowercasing, split categories on prepositions, and remove stopwords. We also split up categories with years or centuries, removing the temporal information: for example, “20th-century atheists” would become just “atheists.” These steps help reduce the frequency of highly specific types that are unlikely to be predictable from context. These post-processed Wikipedia categories become the gold types $t^∗$ associated with each entity mention in our training data. Our final typeset T consists of the 10,000 most frequently occurring types in the training set. See the supplementary material for some examples of common, less common, and rare types.

#### Typing Model

Our model f accepts as input the entity mention m and its context s and predicts probabilities for predefined entity types $T$. Our model is similar to the one in Onoe and Durrett (2020b), which we extend to use pre-trained multilingual BERT (mBERT) (Devlin et al., 2019). Thus, our model accepts input in any language in the mBERT vocabulary. First, we use mBERT as the mention and context encoder. This Transformer-based encoder takes an input sequence of the form `x = [CLS]m[SEP]s[SEP]`, with the mention m and the context s split into WordPiece tokens. We use the hidden vector $h^{[CLS]} \in R$ d at the `[CLS]` token as an intermediate vector representation of the mention and context, where $d$ is the dimension of hidden states. Then, we compute a dot product between $h^{[CLS]}$ and the type embedding matrix $T \in R^{d\times|T|}$ to produce a vector whose components are scores for the entity types $T$.

Finally, we pass the score vector through an element-wise sigmoid function to produce the final probabilities for each type. Each element of the vector corresponds to the model’s confidence that the given input entity belongs to the corresponding type. To get the final set of predicted types for a given mention $m$ and context $s$, we add a type $k \in T$ to the set if the corresponding vector value $t_k$ is greater than a threshold value of $0.5$. 

Following Onoe and Durrett (2020b), the loss is the sum of binary cross-entropy losses over all types T over the whole training dataset D, ignoring type hierarchy in order to reduce model complexity. We predict each type independently and optimize a multi-label binary cross entropy objective. With each iteration through the training data, we update parameters in the mBERT encoder as well as the type embedding matrix.

#### Experiments 
Our focus here is to shed light on the performance of our entity typing model as well as its ability to generalize along two axes: zero-shot languages and unseen entities. To this end, we first compare the performance of our typing model to two baselines (see Section 5.1). We also compare the performance of models trained on single language (e.g., English) with training on multiple languages. Finally, we breakdown our model’s performance on unseen entities. We report macro-averaged precision, recall, and F1 metrics for our experiments.

**Baselines** 

Given the new problem setting we tackle, few suitable baselines exist. Any pre-existing monolingual model will fail to generalize to new languages, and existing multilingual typing methods use much smaller ontologies than our model. With these limitations, we have formulated two comparison methods to contextualize our model’s results

**String-Match Baseline** This baseline tests how well the model can type entities by simply regurgitating types for entities it has already seen, with no disambiguation. We create a dictionary $M$ which maps all entity mention strings m in the training dataset to their most frequent QID, which we then map to the corresponding gold entity types $t^∗$ . At test time, we predict the categories as follows: $M(m)$ if $m$ present in training data $\emptyset$ otherwise.

As expected, this STRING MATCH approach forms a strong baseline for the test languages that are also in the training data: English, German, and Spanish. However, because this baseline only matches the exact string of an entity mention, it fails to generalize effectively to any new languages, especially those that do not use Latin characters. Comparing against string matching specifically highlights our system’s ability to both understand the mention string itself deeply using a multilingual encoder as well as use context around the mention string to make predictions.

**Mention String Similarity Baseline** Exact string match does not handle cases like transliteration; we implement a simple mBERT-based method to do this. For every unique entity in our training set, we encode the corresponding mention string using (non-fine-tuned) mBERT and store the hidden vector at the `[CLS]` token. At test time, we encode each example mention string in the same way, and we then perform a similarity search over the training mention representations using the FAISS library (Johnson et al., 2017). Finally, we predict the categories of the entity with the highest similarity to the training mention representation.

Because we use mBERT to encode our representations, this model can make better predictions in new languages when compared to STRING MATCH, and it can also generalize to unseen entities if there is a semantically similar mention in the training data. However, this baseline does not have access to context.

#### Zero-shot Languages

We measure the model’s performance at entity typing on six new languages: Arabic, Persian, Japanese, Serbian, Spanish, Turkish, and Tamil, evaluating how well it can generalize to new languages without any additional training data in those languages.

![[2021_selvaraj_performance.png]]

Typing Performance Table 2 reports macroaveraged precision, recall, and F1 on the test sets, compared to the STRING MATCH and the MENTION SIMILARITY baselines. Our multi-language entity typing model trained on English, Finnish, German, and Spanish data together (Multi-lang), outperforms the MENTION SIMILARITY baseline on all of the six test languages by a substantial margin, and even more so compared to the STRING MATCH baseline. We compare the Multi-lang model with models trained only on English or Spanish. Notably, our results in Table 2 show that multi-language training boosts performance on entity typing over single-language training, even in completely separate languages, with the exception of Spanish, for which the Spanish model performs slightly better. This fits with Wu and Dredze (2020), which found that high-resources languages often benefit from single-language training, but interestingly, we found that did not apply to English. We also report performance on the training languages for completeness. The STRING MATCH baseline outperforms the model on these test sets in the three languages which overlap with the training set languages: English, Spanish, and German. One possible reason for this is that the entity mentions in Mewsli-9 are often unambiguous, which can be inferred from the high entity linking accuracy by the alias table baseline (Botha et al., 2020). However, unlike the STRING MATCH baseline, the model is also able to generalize to new languages, even those that do not use Latin characters, so it achieves much higher results than the baseline when considering these six new languages. The model outperforms the MENTION SIMILARITY baseline on the all these training languages.


Qualitative Analysis Figure 3 shows an example of the model’s predictions in Spanish, one of the languages in the training data. The model manages to predict the gold types exactly. In contrast, the example in Figure 4 shows an input sequence in an unseen language, Japanese. Although the model does not predict every single type in the gold list for the given entity, it still picks out several of the most salient ones, and notably predicts with 100% precision.

Our multi-language approach can be seen as an effective way to augment training data for entity typing using different languages of data available. Increasing the number of languages increases entity coverage in the training set, which boosts performance. Critically, this works even on unrelated languages. The poor performance of the STRING MATCH baseline on these new languages makes it evident that the entity strings do not directly match languages the model has already seen, but because types can model entities in a language-agnostic way, the model still benefits from seeing more entities during training, and it can generalize that typing knowledge to new languages. This approach may be especially useful for entity typing with lowresource languages.

**Unseen Entities** One of the greatest difficulties in entity typing is predicting types for new entities (i.e., entities that do not appear during training), so we want to isolate these unseen types to test generalizability. In order to have a reasonable dataset to evaluate entity typing performance, we hold out certain entities from the model during training so we could later test on them. We took a random sample of 5288 entities, ensuring that we collected at least 2% of the entities from each test set (i.e., each language). Then, we filtered out every training example referring to any of these 5288 entities to train a new model. During evaluation, we filtered the test sets to only contain examples that referred to these 5288 entities. We denote these filtered sets as $D'_n$ , where $n$ represents the language.

**Qualitative Analysis** When the model’s predicted types do not match the gold types, they generally still provide sensible information about the entity in question. For example, in the example in Figure 5 with an unseen entity, the model predicts “in india" instead of “indian films." Some of our model’s performance in Table 2 clearly comes as a result of simply memorizing common entities in the training data; however, this memorization is not necessarily a bad thing if the model can still generalize to new languages and new entities like this one that did not appear during training. In some cases with unseen entities, our typeset is not exhaustive enough to cover an entity, so the gold types are empty, as seen in Figure 6. However, our model is still able to predict types that are semantically relevant. For example, in this example, “causes of death" is a valid type when considering the mention in conjunction with its surrounding context. This example shows that despite the noisy, distantly-supervised nature of the data, the model does learn to predict types contextually.

**Rare Types** 
We finally break down entity typing performance by the type frequency ranking in three buckets: [0, 99], [100, 999], and [1000, 9999]. We report macro-averaged precision, recall, and F1 metrics in Table 5, both on the full dataset and on unseen entities. For the languages in the training data (English, German, Spanish), entity-typing performance is relatively high across all three buckets. The model also achieves good results on unseen languages for frequent types. Most of the categories in this first bucket are more coarse-grained, corresponding to certain locations or professions, and the model performs relatively well at distinguishing these types. Next, we look at the type-frequency breakdown of entity typing limited to unseen entities. For predicting the rarest types on unseen entities, performance drops off in the second and third buckets for certain languages, such as Tamil, Serbian, Arabic, and Persian. However, the model is able to predict rare types on unseen entities in other unseen languages, such as Japanese and Turkish. Although the numbers here are hard to draw strong conclusions from, given the results of the previous human evaluation, they show some success even on the


---

Nila Selvaraj, 
Yasumasa Onoe, 
Greg Durrett 

Department of Computer Science The University of Texas at Austin