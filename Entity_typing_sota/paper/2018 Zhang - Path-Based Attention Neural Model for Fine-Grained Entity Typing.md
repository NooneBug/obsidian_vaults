https://ojs.aaai.org/index.php/AAAI/article/view/12162

#### Abstract
In this paper, we propose an end-to-end typing model, called the path-based attention neural model (PAN), to learn a noise-robust performance by leveraging the hierarchical structure of types

#### Introduction
The training data is generated by distant supervision, which assumes that if an entity has a type in knowledge bases (KBs), then all sentences containing this entity will express this type. This method inevitably introduces irrelevant types to the context. For example, the entity “Donald Trump” has types “person”, “businessman” and “politician” in KBs, thus all three types are annotated for its mentions in the training corpora . But in sentence “Donald Trump announced his candidacy for President of US.”, only “person” and “politician” are correct types, while “businessman” can not be deduced from the sentence, thus serves as noise. To alleviate this issue, a few systems try to denoise training data by filtering irrelevant types ahead of training. For instance, (Ren et al. 2016) proposes PLE to identify correct types by jointly embedding mentions, context and type hierarchy, and then use clean data to train classifiers. However, the denoising and training process are not unified, which may cause error propagation and bring much additional complexity.

Motivated by this, we propose an end-to-end typing model, called the Path-based Attention Neural model (PAN) to select relevant sentences to each type, which can dynamically reduce the weights of wrong labeled sentences for each type during training. This idea is inspired by some successful attempts to reduce noise in relation extraction, e.g.,(Lin et al. 2016). However, these methods fail to consider type hierarchy, which is distinct in fine-grained entity typing. Specifically, if a sentence indicates a type, its parent type can be also deduced from the sentence. Like the example above, “politician” is the subtype of “person”. Since the sentence indicates that “Donald Trump” is “politician”, “person” should also be assigned. Thus, we build path-based attention for each type by utilizing its path to its coarsest parent type (e.g., person) in the type hierarchy. Compared to the simple attention in relation extraction, it enables parameter sharing for types in the same path. With the support of hierarchical information of types, it can reduce noise effectively.

#### Path-Based Attention Neural Model

![[2018_zhang architecture.png]]

The sentence encoder is the [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|Shimaoka's one]], then attention $\alpha_{1, t}$ on each type $t$ is used to obtain a weighted sum $s_{e, t}$ of the single token embeddings $s_i$  

Also types are represented; each type has an embedding $t_l$ and the type representation $p_{t_l}$ is obtained by the composition of the type representation $t_l$ and the representation of all $t_l$ ancestors. In this way, hierarchical related types will share parameters for type representation. It makes learning easier especially for infrequent subtypes, which suffer from dearth of training data, since the attentions for these subtypes can get support from the attention for parent type.

#### Experiments

The approach is experimented on [[Dataset - Ren's Ontonotes]], [[Performances - Ren's Figer]]




---

Denghui Zhang, 1 
Manling Li, 1 
Pengshan Cai, 2 
Yantao Jia, 1 
Yuanzhuo Wang, 1

1 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China
2 School of Computer Science, University of Massachusetts Amherst, MA 01003

#paper