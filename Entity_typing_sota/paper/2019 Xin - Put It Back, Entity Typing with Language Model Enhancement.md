https://aclanthology.org/D18-1121.pdf

Abstract
---
Most existing models obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose entity typing with language model enhancement. It utilizes a language model to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels.

Introduction
---
we propose Entity Typing with Language Model Enhancement (LME). It is able to measure the compatibility between the context sentence and each distantly supervised label, in an unsupervised manner using meaning of the label

We argue that, the meaning of entity mention words can also be expressed by the name of its context-dependent type, to some extent. Based on this argument, replacements with context dependent types make more sense than those with global-but-context-irrelevant ones (Table 1)

![[2019_Xin_table_1.png]]

Our method employs a language model to evaluate the soundness of each synthetic sentence generated by replacing the entity mention with its typeâ€™s name. It is able to focus more on context-dependent types

Model
---
![[2019_xin_architecture.png]]

The model is composed of two modules: Entity Typing Network (ET) and Language Model Enhancement (LME).

ET simply predict a score for each type.

LME takes in input the predictions of ET and evaluates the soundness, so backpropagates the information through ET. During inference LME is NOT used, 

[[Inference Method - Threshold or Max]] is used as inference method on ET output.

**Encoder**
- Entity mention is encoded as the average of the entity's words #pretrained_word_embeddings_glove 
- Context is encoded with two BiLSTM #encoders_with_recurrent_architecture  with self-attention #encoder_with_attention 
- Handcrafted features from [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]] and [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]]

Then a linear layer produces a score for each type. The network is trained with BCE

**LME**
LME is a LSTM pretrained on the corpus, in order to have a domain specific language model. 

LME is used to evaluate the soundness of the sentence with a label instead of the entity tokens. The soundness is the average probability of the next word probabilities extracted in each step of the LSTM.

Each type is represented with a learnable vector, initialized with the word embedding of the type's label.

The hybrid sentence (the one with the type instead of the entity token) is represented by encoding all predicted types as the sum of all predicted type's vectors.

During the training only ET is updated.

Experiments
---

Approach is experimented on [[Dataset - Ren's FIGER]] and [[Dataset - Ren's Ontonotes]]

___

Ji Xin1,2, 
Hao Zhu1 , 
Xu Han1 , 
Zhiyuan Liu1 , 
Maosong Sun1 

1State Key Laboratory on Intelligent Technology and System Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, China 

2David R. Cheriton School of Computer Science, University of Waterloo, Canada

#paper #encoder_with_attention #encoders_with_recurrent_architecture #pretrained_word_embeddings_glove 