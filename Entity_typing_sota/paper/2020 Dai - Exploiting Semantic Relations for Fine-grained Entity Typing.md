https://openreview.net/pdf?id=BSUYfTada3

https://github.com/HKUST-KnowComp/SRFET

"*We find that, in some cases, existing neural fine-grained entity typing models may ignore the semantic information in the context that is important for typing. To address this problem, we propose to exploit semantic relations extracted from the sentence to improve the use of context. The used semantic relations are mainly those that are between the mention and the other words or phrases in the sentence. We investigate the use of two types of semantic relations: hypernym relation, and verb-argument relation. Our approach combine the predictions made based on different semantic relations and the predictions of a base neural model to produce the final results.*"

"*The hypernyms of mentions occur frequently when the referred entities are newly occurred or not known by a lot of people, which makes the typing results based on them especially valuable*"

This article underlines the usefulness of explicitly find hypernym relations and verb-relations between words in the input and then add this information during the training/inference

"*Thus, we believe that it is possible to improve neural FET models by using some semantic relations between the mention and the context words or phrases. In this paper, we investigate how the two types of semantic relations that is already discussed above can be exploited in FET: hypernym relations, and the verb-argument relations obtained with semantic role labeling. To utilize them effectively, we design two individual components that performs FET based on these two types of relations respectively*"

"*(1) We are the first to propose using the semantic relations between the mention and some words or phrases in the sentence to improve fine-grained entity typing. A framework that can incorporate different types of such relations is proposed. (2) We investigate the use of hypernym relations and verb-argument relations for FET*"

![[2020_dai_architecture.png]]

The approach is designed as follows: 

- Entity Typing Network (called "base model"): authors tries different Bert-Based models; and use the best one as Entity Typing Network
- Hypernym Relation Based FET: a module that uses hypernym information to produce a prediction of each type
- Verb-Argument Relation Based FET: a module that uses relation information to produce a prediction of each type

The predictions are averaged OR weighted averaged (weights obtained by a Gated Network)

Authors tries different setups on [[Dataset - Ren's BBN]] and [[Dataset - Ren's FIGER]]:

- BERT-Based base model with only linguistic information
	- FIGER: .8677 | .8417
	- BBN :   .9062 | .9099
- BERT-Based base model with entity linking information (from [[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking | 2019 Dai]]):
	- FIGER: .8909 | .8663
	- BBN :   .9147 | .9170

#paper 
#entity_linking