https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9706297

### Abstract  
Fine-grained entity typing (FGET) is an important natural language processing (NLP) task. It is to assign fine-grained semantic types of a type taxonomy (e.g., `Person/artist/actor`) to entity mentions. Fine-grained entity semantic types have been successfully applied in many natural language processing applications, such as relation extraction, entity linking and question answering. The key challenge for FGET is how to deal with label noises that disperse in corpora since the corpora are normally automatically annotated. Various type taxonomies, typing methods and representation learning approaches for FGET have been proposed and developed in the past two decades. **This paper systematically categorizes and reviews these various typing methods and representation learning approaches to provide a reference for future studies on FGET. We also present a comprehensive review of type taxonomies, resources, applications for FGET and methods for automatically generating FGET training corpora. Furthermore, we identify the current trends in FGET research and discuss future research directions for FGET. To the best of our knowledge, this is the first comprehensive review of FGET.**

## Introduction
It is essential to recognize entities such as names (e.g., names of person, organization and location) and numeric expressions (e.g., time, date, money and percent expressions) in natural language text. Identifying references or mentions of these entities in text was acknowledged as an important natural language processing (NLP) task in the Sixth Message Understanding Conference (MUC-6) ([[1996 Grishman - Message Understanding Conference - 6, A Brief History|MUC-6, a brief history]]). Named Entity Recognition (NER) is to recognize and categorize named entities into coarse-grained classes (typically 4 classes: `Person, Location, Organization and Miscellaneous` ([[2003 Sang - Introduction to the CoNLL-2003 Shared Task, Language-Independent Named Entity Recognition|CoNNL]])). However, this kind of coarse-grained type information is insufficient for many applications, while fine-grained entity types can lead to substantial improvements on many NLP tasks ([[Downstream task in which ET is usefull]]). Thus, there has been a booming research stream on Fine-grained Entity Typing (FGET), which is to classify entity mentions into more fine-grained semantic types. For example, *Leonardo DiCaprio* is typed as `person/artist/actor`. The mentions of entities are complicated. A large proportion of entity mentions are coreferences (alias, nickname, pronoun, abbreviation, etc.) and ambiguous names. Furthermore, quite a number of entity mentions in textual documents are new entities that are not included in a knowledge base (KB). Thus, several tasks of entity analysis were developed to tackle these problems. For example, Entity Mention Detection (EMD) is to identify all the mentions of entities, including names, noun phrases and pronouns that refer to entities; Entity Coreference Resolution [14] is to identify and cluster entity mentions that refer to the same entity; Entity Linking is to resolve the ambiguity of an entity mention by linking it to a specific entity in a KB. All these tasks, along with FGET, are clustered as entity analysis. 
These entity analysis tasks are highly interdependent. Thus, an approach for FGET may tackle FGET with other tasks simutaneously [18], [19], [20], [21], [22], [23], [16], [24]. For example, Durrett and Klein [16] proposed a model that tackles multiple tasks jointly by taking advantage of cross-task interactions between FGET, entity coreference resolution and entity linking using a structured conditional random field (CRF).

**Motivations for this review:** Fine-grained type information of entities (mentions) have been successfully applied in many NLP tasks. FGET has attracted significant attention, and a considerable number of approaches with different type taxonomies have been proposed. However, there are no reviews on FGET approaches and related issues. To the best of our knowledge, **this is the first comprehensive review of FGET**

**Contributions of this review:** We systematically categorize, compare and analyze various typing methods, representation learning approaches. The other issues, such as KB resources, FGET applications, training corpora generation and type taxonomies, are also reviewed to provide useful resources for the FGET research community. Further, we identify the current trends in FGET research: (i) Learn embedded feature representations to address the challenges posed by label noises, tail types and new entities; (ii) Tackling FGET and other entity analysis tasks jointly is also a promising direction.

## Background

In this section, we first give a formal definition of the FGET task, and then review the applications of FGET as well as the KB resources for FGET.

##### Task Definition

Given a document $d$ or a collection of documents $D$ that contains a set of entity mentions $M$, and a predefined taxonomy of fine-grained semantic types $T$, the task of FGET is to assign each entity mention $m \in M$ an appropriate type $t \in T$ on the type taxonomy. An entity may have multiple types in different contexts, e.g., *Donald Trump* is a political figure, a businessman and an actor. FGET can be divided into two kinds of tasks: *mention-level FGET* and *entity-level FGET*. The formal definitions are given as follows:
- **Entity Mention:** Entity mention is a continuous span of tokens in the text which refers to a real world entity. Entity mention can be a named entity mention, a nominal mention or pronoun coreference.
- **Type Taxonomy:** Type taxonomy or type ontology is a tree or a directed acyclic graph (DAG) $O = (T, R)$, where $T$ is the set of semantic types and $R$ is the edge set. $R = {(t_i , t_j ) | t_i , t_j \in T, i \neq j}$ is also called the relation set, in which $(t_i , t_j )$ means that $t_j$ is a sub-type of $t_i$ .
- **Mention-level FGET:** Mention-level FGET can be defined as $f : M × C \rightarrow T$ ($C$ is the set of the corresponding context of each mention in $M$), which is to find a semantic type with the appropriate degree of granularity for an entity mention within a specific context. Mention-level FGET is also called context-dependent FGET.
- **Entity-level FGET:** Entity-level FGET can be defined as $f : M × D \rightarrow T$, which is to find all possible semantic types for an entity. The type set of an entity should be the union of mention-level FGET in different contexts. Entity-level FGET is also named context-independent FGET or corpus-level FGET. Entity-level FGET is mainly motivated for KB construction and completion.

An example of mention-level FGET is presented in Fig. 1. *Donald Trump* has multiple types, including political figure, business and TV show actor. But from the context given by the sentence, `Person/political figure` is the appropriate type path, so the type label could be `Person` or `political figure`.
![[2022_wang_fig_1.png]]

For entity mentions, some approaches [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]], [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions]], [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing]], especially those entity-level FGET methods, assume that the gold entity mentions are segmented by NER or EMD tools. Some methods view FGET as fine-grained NER, treat the mention detection as an integrated part of FGET, and organize the mention detection and entity typing in a pipeline way [[2012 Ling - Fine-Grained Entity Recognition]], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]]. Some methods for automatically generating training data also work in this way. A detailed summary of mention detection in automatically creating training corpora is given in Section 3.2.1

Entity-level FGET is related to word clustering [30] (or many other names, e.g., semantic class identification, hyponym acquisition [31], semantic lexicon induction, semantic class learning [32]), which is to assign words to classes based on the statistics from large corpus. This task originates from the class-based n-gram language model [33], which reduces the problematic effects of sparsity by grouping similar words into classes. Mention-level FGET is similar to word sense disambiguation (WSD) [34]. WSD is a task of determining the fine-grained semantic class of words in contexts. Both mention-level FGET and WSD need to make decisions based on the contexts of mentions or words. **Compiling semantic type taxonomy is related to the automatic ontology construction, which aims to create lexical hierarchies based on semantic classes [35]. However, type taxonomies used for FGET were manually curated or semi-automatically mapped from type taxonomies of a KB.**

## Type Taxonomy and Training Corpus

In this section, we present a review of how the type taxonomies are curated and how the training corpora are automatically generated.

![[2022_wang_taxonomies_table.png]]

**Type Taxonomies** 
FGET uses far more complex type taxonomies as opposed to traditional NER. There are two categories of type taxonomies: manually curated and semi-automatically curated. 

**Manually Curated Type Taxonomies** Initially, Locations were manually classified into 8 fine-grained subtypes (i.e., *Country, City, Street, Territory, Region, Water, Mountain*, and *Artifact*) [[2001 Fleischman - Automated Subcategorization of Named Entities]] and Persons were classified into 8 subcategories [[2002 Fleischman - Fine Grained Classification of Named Entities]] (i.e., *Athlete, Politician/Government, Clergy, Businessperson, Entertainer/Artist, Lawyer, Doctor/Scientist, Police*).

Sekine et al. [72] compiled a taxonomy of 150 types (extended to 200 types [73] later) with maximum depth 5 to make it broad enough for general applications. They used three methods (i.e., based on corpus, based on previous systems and tasks, and based on thesaurus) to design three initial taxonomies. Then they merged the three taxonomies into one and refine it. Lee et al. [42] introduced a set of 147 fine-grained types with two levels (the 15 top-level types include Person, Study Field, Theory, Artifacts, Organization, Location, Civilization.) for question answering

**Semi-automatically Created Type Taxonomies** 
KBs have been used for semi-automatically generating type taxonomies. For example, Wikipedia categories have been used to derive a taxonomy [86], [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments]]. Those categories whose syntactic heads cannot be found in WordNet [58] are removed, and types are obtained by mapping categories to their syntactic heads. Compared with the Sekine type taxonomy [73], Wikipedia category based type taxonomy is more like a semantic tag than a well-defined type taxonomy, while Freebase semantic types provide broader coverage of entities and types, and allow for multiple types being assigned to one entity. But the Freebase types are noisy since they are labelled by non-expert volunteers. Thus, [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] took two steps to get a well-defined type taxonomy [[Dataset - FIGER|FIGER]] from Freebase types: (i) Irrelevant types are filtered out, and only those types with more than 5 ground instances in Freebase were kept; (ii) Those too specific types were manually merged. Similarly, the GFT ([[Dataset - Ontonotes v5 - GFT]]) type taxonomy [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]] was obtained by first organizing the non-hierarchical types (the level 2 types) of FIGER into a hierarchy and then refining them by discarding rare or ambiguous types. New types were inserted if there were enough instances to corroborate the insertion. Although the coverage and specificity limitations of Freebase types may be exacerbated by such manually merging, FIGER and GFT are two of the most widely used taxonomies.

The semantic types of other KBs are leveraged to derive type taxonomy as well. For example, [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|Hyena]] types were derived from [[Dataset - YAGO505|505 YAGO]] types. The 22 top-level types of [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]] were derived from the ontology of DBpedia [65]. PEARL [[2013 Nakashole - Fine-grained semantic typing of emerging entities|2013 Nakashole]] used the YAGO2 [71] type taxonomy, which was derived from 200 WordNet classes.

The ultra-fine grained type taxonomies normally have thousands of types. For example, FINET [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|2015 Del Corro]] consists of more than 16K types with top categories Person, Location and Organization. [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]] organized types into three disjoint bins: 9 general types, 121 fine-grained types and 10,201 ultra-fine grained types. Murty et al. [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017]] [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018]] manually mapped Freebase types to the WordNet hierarchy, and got a type taxonomy of 1941 types with average depth 7.8. The purpose of introducing ultra-fine-grained types is to improve the coverage and diversity of type taxonomies, and to alleviate the skewed label distributions of corpus which is heavily skewed toward coarse-grained types. However, the taxonomy with thousands of types also poses a great challenge to typing models. There are also domain-specific type ontologies. For example, Unified Medical Language System (UMLS) [89], a biomedical hierarchical concept ontology with average depth 14.4, contains over 3.5 million concepts. A summary of FGET type taxonomies is listed in Table 1. As we can see, extensive work has been done to curate type taxonomies for FGET. However, no agreement has been reached by the research community, as the granularity of semantic types is quite subjective and dependent on task needs.

### Training Corpora Generation

Manually annotating data with multiple fine-grained types is a daunting task. Thus, almost all the FGET training corpora are generated automatically, i.e., mentions are linked to a specific entity of a KB using an entity linking tool (such as Dbpedia Spotlight [90] or AIDA [91]) or anchor links in the Wikipedia, and they label the mentions with all types from the KB. As for the mentions, some mention-level FGET methods [26], [27], [[2017 Yaghoobzadeh - Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities|2017b Yaghoobzadeh]] and all methods of entity-level FGET treat mention boundaries as given ground truth. There exist publicly available labelled corpora, such as Wikilinks [92] and ClueWeb [93], in which entity mentions are identified and linked to a KB. In the following subsections, we review the issues involved in automatically generating training corpora.

**Entity Mention Detection** 
Some works on FGET, especially those on mention-level FGET, treat mention boundaries detection as an integrated part of automatically generating training examples. Nominal, pronominal, and named entity mentions are all considered as targets. FINET [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|2015 Del Corro]] directly uses Stanford CoreNLP tool to extract named entities with coarse-grained types. Thus, only the named entities are extracted, and the nominal phrases and pronouns are ignored. [[2013 Nakashole - Fine-grained semantic typing of emerging entities|Nakashole et al.]] proposed a method to detect and type noun phrases that refer to new entities. However, this method cannot be used to generate training examples directly, because it can only generate type labels in open vocabulary words and these labels need to be converted to type labels of a hierarchical taxonomy. To extract both named entity mentions and noun phrase, [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] applied a POS3 tagger, a dependency parser and a noun phrase (NP) extractor in a pipeline way, where the noun phrase extractor relies on POS tags and dependency edges to detect entity mentions. But the pronominal mentions are ignored as there is no coreference resolution processing. [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] treated the segmentation of mentions as a sequence labelling task and trained a linear-chain CRF model on a heuristically-labeled Wikipedia data to detect entity mentions. Ren et al. [94] used quality mention examples from a KB as distant supervision and a random forest classifier to score segmentation quality. Many approaches [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments|2017 Rabinovich]] use the anchor links in Wikipedia to detect entity mentions, but only less than 3% of text tokens in Wikipedia are anchored strings. Ghaddar and Langlais [61] proposed finding more non-anchored entity mentions by leveraging: (i) The out-link structure of Wikipedia, i.e., search the titles of the out-links in the current article; (ii) The coreferent mentions that refer to the main concept of a Wikipedia article extracted by WikiCoref [95], [96]. This can raise the coverage of mentions from less than 3% to about 30%.

**Corpora Automatic Labelling**
One approach for obtaining semantic types of a mention is using Wikipedia anchor links [60]. For each linked mention in a sentence, [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] and [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]] found the corresponding Wikipedia entry via the anchor link; the semantic types were then mapped from Freebase or DBpedia to their own type taxonomy. WiFiNE [61], a large scale FGET corpus generated from Wikipedia, automatically maps the corresponding Freebase object type of each entity to GFT type taxonomy [29] or FIGER taxonomy [8]. The Wikipedia’s categories are not used since they are more like tags than a well-defined type taxonomy. The disadvantage of using anchor links is that the number of training examples is limited by the Wikipedia anchor links.

Another method of obtaining semantic types of a mention is using entity linking tools. [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|Yogatama et al.]] and Ren et al. [94] used an entity linking system to retrieve Freebase types of resolved entity mentions, and then mapped Freebase types to their taxonomy. They used heuristics to reduce label noises. However, the linking errors can introduce label errors in the training examples. Moreover, both the Wikipedia link method and the entity linking tool method fail to label out-of-knowledge-base entities. The appositional structures are used to generate training examples of out-of-knowledge-base entities. Ekbal et al. [97] leveraged appositional patterns to extract person name-type pairs from ukWaC corpus. They used pattern frequency and WordNet information to reduce ambiguous and invalid semantic types. This method can achieve a 96.58% accuracy. But the number of examples is limited by the sparseness of apposition and the type is limited to Person

For [[2018 Choi - Ultra-Fine Entity Typing|ultra-fine labeling]], head words were directly used as type labels. However, head words labels are scarce and not context-sensitive. Thus, [[2021 Dai - Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model|Dai et al.]] proposed a method that combines hypernym extraction patterns [31] and a pretrained language model to generate ultra-fine labels .

**Label Noise Reduction**

The automatically generated type labels have two kinds of label noises: [[Noise Definition - Out-of-context|out-of-context]] noises and [[Noise Definition - Overly Specific|overly-specific]] noises [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu et al.]]. To explain this, consider the mention Hugh Laurie in such two sentences: (i) "*Hugh Laurie* and his wife Jo Green were on the verge of divorce."" (ii) "*Hugh Laurie* wins Best Supporting Actor in a miniseries". *Hugh Laurie* has multiple labels (e.g., *actor, director, musician, comedian, and author*) in a KB, and all these labels will be assigned to both mentions. In Sentence (i), the type label should be *Person*, all other types are *overly specific* label noises (they are not out-of-context labels, since they are coherent with the context). In Sentence (ii), the type label should be `Person/artist/actor`, and all the other types are out-of-context label noises.

Using training examples with such label noises, some approaches ignore the noises and treat training corpus as normal [8], [26], [100], [101]. The performance of these approaches is decreased by such label noises. 

Another approach is to devise mechanisms to deal with the noises, to either directly tweak the noisy labels, such as [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [77], [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]]; or enhance the typing model with the ability of tackling label noises, such as [[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding|Ren 2016a]], [[2016 Ren  - Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding|Ren 2016b]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|Abishek 2017]], [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu et al.]]. These mechanisms are reviewed as follows:

**Out-of-Context Label Noise Reduction** 

[[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]] used head words as distant supervision to avoid out-of-context noisy labels. For example, for mention “the 44th president of US”, a KB may provide multiple types such as *author, lawyer*, and *president*. Head words can reduce the type set to *president*. But such distant supervision is not always available, thus most of the noisy labels will not be reduced. 

[[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] used the following two heuristics to reduce out-of-context noisy labels: (i) coarse type pruning: deleting type labels that are inconsistent with the prediction of a standard NER classifier trained on ACE [79] data; (ii) minimum count pruning: removing type labels that appear only once in a document. However, such heuristics may cause some mentions to have ‘none’ labels.

[[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding|AFET]] separates the loss functions for clean (with single type label) and noisy (with multiple type labels) examples, and uses clean examples to de-noise those examples with noisy labels. But the performance is sensitive to parameters, which are corpus dependent. To obviate the need for tuning parameters, [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|Abishek 2017]] proposed a neural network model with a [[Special Hinge Loss - 2017 Abishek|variant of hinge loss function for noisy examples]]. The model makes the maximum score for the set of positive but noisy types greater than one. But the possible semantic correlations among types were ignored during the training of type embeddings.

**Overly-Specific Label Noise Reduction** 

[[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] used the so called sibling pruning to heuristically remove overly-specific labels, i.e., sibling labels would be removed if they share a single parent type. For example, a mention labelled with `/person/artist/director` and `/person/artist/actor` would be reduced to `/person/artist`. The performance of their typing model has noticeable improvement after reducing the noisy labels using this heuristic. But this also removes training examples for non-prominent types.

**General Label Noise Reduction** 

WiFiNE [61] uses the following two heuristics to de-noise the labels: (i) the type arguments of Freebase relations, e.g., the `place_of_birth` relation indicates that the type of the first argument is person; (ii) the sharing of common attributes, e.g., if two entities appear in the same sentence, and the non-noisy mention has a type set which is a subset of the noisy mention, then the noisy label is replaced with the non-noisy label. Such heuristics may keep noisy labels while removing true labels.

[[2016 Ren  - Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding|Ren 2016b]] used unambiguous mentions (only one type label) as distant supervision to de-noise those mentions with multiple labels that appear in a similar context. The mentions, contextual words and type labels, were embedded into a common dimensional space by optimizing a margin-based rank loss function. Because the noise reduction step was separated from the typing model, the errors engendered by noise reduction will be propagated into the typing model.

To alleviate this phenomenon,  [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu et al.]] proposed a simple variant of cross-entropy loss to handle data with out-of-context noises, and introduced a hierarchical loss normalization process to let the model understand the type hierarchy and mitigate the negative effects of overlyspecific noises. But the hierarchical loss normalization parameters need to be tuned on a different corpus.

**Label Noise Reduction for Entity-level FGET** 

Entity-level FGET methods [[2017 Yaghoobzadeh - Noise Mitigation for Neural Entity Typing and Relation Extraction|2017c Yaghoobzadeh]], [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017 Murty]], [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]] use multi-instance multi label (MIML) learning [105] to reduce the negative effects of label noises. MIML operates over a bag of mentions (collection of all sentences/contexts containing the same entity) rather than over a single mention. For MIML, at least one mention of a bag justifies the types given in the KB, and one bag can have multiple types. But this may amplify the dominance of prominent entities with prominent types, and make the prediction on tail types more difficult.

## Feature Representation

In this section, we present a comprehensive review of feature representations for FGET.
![[2022_wang_table_2.png]]

#### **Hand-crafted Features**


The following categories of hand-crafted features have been exploited for FGET: Mention-level, Context-level, Document-level, Corpus-level, Gazetteers, External and Type-based.

Mention-level features include: syntactic head, non-head token, POS tags, word shape, characters, and prefix/suffix. Brown cluster id [33], [30] of all words [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] or head word [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], WordNet synset of words [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|2015 Del Corro]] are used as well.

Contextual features used in FGET include: n-gram of words around the mention phrase, POS tags of contextual words. The syntactic dependency features, such as the role of the mention head on the dependency tree [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], the lexical parent of the mention head [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], the verb governing an entity mention [[2010 Rahman - Inducing Fine-Grained Semantic Classes via Hierarchical and Collective Classification|2010 Rahman]], are also used in FGET.

Topic features are from a simple bag-of-words topic model with eight topics (e.g., arts, business, entertainment etc.) [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], or unigrams, bigrams, and trigrams extracted from a paragraph window [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|2012 Josef HYENA]].

Gazetteer features signify whether the mention contains a word in a type’s gazetteer or not. Gazetteers are from WordNet hyponyms [[2010 Rahman - Inducing Fine-Grained Semantic Classes via Hierarchical and Collective Classification|2010 Rahman]], YAGO [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|2012 Josef HYENA]] or Web data [[2011 Tkatchenko - Classifying wikipedia entities into fine-grained classes|2011 Tkatchenko]].

External features include: WordNet being used to expand context features [[2002 Fleischman - Fine Grained Classification of Named Entities|2002 Fleischman]], Wikipedia articles [[2016 Suzuki - Fine-grained named entity classification with wikipedia article vectors|2016 Suzuki]].

Type-based features [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments|2017 Rabinovich]] consider the compatibility between the mention and the types in the predicted type set, and the relation and compatibility among the types in the predicted type set.

Table 2 lists the hand-crafted features used in FGET. However, there is paucity comparison of the effectiveness of these features. [[2010 Rahman - Inducing Fine-Grained Semantic Classes via Hierarchical and Collective Classification|Rahman and Ng]] analyzed the effects of their seven types of features. Their results show that the best accuracy is achieved when only features of mention tokens, WordNet synsets and POS tags of mention tokens are used. This indicates that their gazetteers, morphological features, capitalization, and the semantic role of governing verb are redundant. They also found that the most useful features are the mention tokens. While [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|Yosef et al.]] found the performance drops significantly when the gazetteer features are disabled.

The hand-crafted features represent the human knowledge on FGET, but the feature independence assumption is too loose. These features are often represented as dimensional one-hot vectors; therefore the typing model will inevitably suffer from the curse of dimensionality. Moreover, most of these features are generated by NLP tools, the errors of NLP tools will definitely propagate into typing models.

#### **Embedded Features**

Since the successful applications of neural network in natural language processing [113], [114], [115], feature embeddings have been an effective way of learning and representing features. Most FGET approaches use the pre-trained word embeddings as input, e.g., GloVe [116] is used in [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu]], [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|2017 Karn]], [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention|2018 Xin]], [[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds| 2018 Zhang]], [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]], [[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing|2019 Xiong]], Word2Vec [121] and fastText [122] are used in [[2016 Jia - Fine-grained Entity Typing for Knowledge Base Completion|2016 Jia]], [[2016 Ma - Zero-shot dataset partition|2016 Ma]], [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|2017 Yaghoobzadeh]].

**Mention Embedding**
Mention embeddings encode the compositional, morphological and orthographic information about entity mentions. For convenience, we represent the words in a mention as $m_1, m_2,$ . . . ($m_{|m|}$ , $|m|$ is the length of mention span), and $u$ is a mapping function from tokens to embeddings. 

**Averaging Encoder** 
Compute the mention embeddings by averaging embeddings of words that comprise the mention phrase [[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification|2016 Shimaoka]], [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]], [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu]], [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|2017 Yaghoobzadeh]], [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|2017 Karn]], [[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds| 2018 Zhang]], [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention|2018 Xin]], [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017 Murty]], [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]], [[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking|2019 Dai]], [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|2019 Obeidat]], [[2020 Chu - ENTYFI, A system for fine-grained entity typing in fictional texts|2020 Chu]].  $v_m = \frac{1}{|m|}\sum_{i=1}^{|m|}u(m_i)$. Variant forms include: a learnable n-grams encoder [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|2017 Yaghoobzadeh]], and a non-linear function on the averaged mention embeddings [[2016 Jia - Fine-grained Entity Typing for Knowledge Base Completion|2016 Jia]]. This relatively simple method for mention embedding is less prone to overfitting [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]]. But this method cannot capture the internal structure and compositional nature of the mention phrase, and the word orders are ignored.

**Attentive Encoder** 
Some words in an entity mention may provide more useful information for typing, e.g., the head words. [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|Lin and Ji]] used the self-attention mechanism to compute the weight of each word.

**Word-level RNN** 
[[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|Dong et al.]] used the recurrent neural network (RNN) [128] to encode the internal structure and compositional nature of entity mentions. [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|Xu and Barbosa]] applied Word-level LSTM [129] to the mention sequence from left to right, and the last output $h_{|m| + 1}$ is used as mention embeddings. The single directional RNN can capture the word order information, but it cannot capture the character-level patterns of words, especially the out of vocabulary words.

**Character-level CNN** 
[[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|Yaghoobzadeh and Schutze]] applied a convolutional neural network (CNN) [130] on the sequence of mention characters. Each character has an embedding in a lookup table. Then max pooling is performed on the output of the CNN filters. This is to capture the character patterns of entities of the same type

**Character-level LSTM** 
[[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|Abishek et al.]] decomposed an entity mention phrase into character sequence and the spaces between tokens are conserved. A vanilla LSTM was then applied to the character sequence from left to right and the final output was used as the mention embedding.

**Multi-level Representations** 
Character-level and word-level information can give complementary information about mentions. [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]], [[2019 Lopez - Fine-Grained Entity Typing in Hyperbolic Space|Lopez ´ et al.]] and [[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing|Xiong et al.]] used the concatenation of character-level CNN and weighted sum of word embedding as the mention embedding. [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|Yaghoobzadeh and Schutze]] explored the combination of word-level averaging vector and character level CNN, LSTM, BiLSTM, and experiments show that character-level CNN is the most effective.

**Context Embeddings**
Context representations capture information about the context surrounding an entity mention. The context can be the whole sentence where the mention phrase appears [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|Abishek et al.]], [132], or a fixed sized context window [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]], [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|2017 Karn]].

Some approaches [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|Xu and Barbosa]], [[2018 Choi - Ultra-Fine Entity Typing|2018 Choi]], [[2019 Lopez - Fine-Grained Entity Typing in Hyperbolic Space|2019 Lopez]], incorporate the position embeddings to reflect the relative distance between context words and mention phrase, i.e., $c_i = c^d_i , c^p_i$ , where $c^d_i , c^p_i$ are word embedding and position embedding of $i$th context word, respectively

**MLP Encoder** 
[[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|Dong et al.]] encoded the left context and right context separately using two Multi-Layer Perceptrons (MLPs) with single hidden layer, then the outputs of the two MLPs were concatenated as context embedding. But this context encoder can only process fixed-size context.

**Word-level CNN** 
[[2016 Jia - Fine-grained Entity Typing for Knowledge Base Completion|Jia et al.]] inserted `〈et begin〉, 〈et end〉` before and after a mention respectively, and used CNN to encode the context. [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017 Murty]] [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]] performed max-pooling operation on the CNN outputs. CNN is good at detecting key phrase patterns, but performs poorly in capturing the semantic relations between mention and context words.

**Word-level LSTM/GRU Encoder** 
[[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification|Shimaoka et al.]] applied Uni-directional LSTM to the left and the right context separately, in reverse direction. The final hidden states of the two LSTMs were concatenated as context embedding. [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|Abhishek et al.]]  applied Bi-directional LSTM to the left and right context separately. The final outputs of both directions were concatenated as feature representation of each context. [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|Karn et al.]] applied a Bi-directional GRU [133] on the left and right context separately. While [[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking|Dai et al.]] used a special token $w_m$ to replace the mention in the context and applied two layers of BiLSTMs to the whole context. The RNN based encoders use the final hidden state or output as context embeddings. This may lose key information for typing, especially when the key word lies at the middle of the context. Thus, the attention mechanism [134], [135] is incorporated to discriminate key information for typing.

**Attentive Encoder** 
[[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification|Shimaoka et al.]] and [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|Obeidat et al.]] explored the self-attention mechanism on their Bi-LSTM encoder, and treated the left and right context separately. $v_c = \sum _i \alpha^l_ih^l_i + \sum _j \alpha^r_jh^r_j$ where $h^l$ and $h^r$ are the Bi-LSTM output of the left and right context, respectively, $\alpha$ are the attention weights. Some similar methods do not separate the left and right context [[2018 Choi - Ultra-Fine Entity Typing|2018 Choi]], [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu]], [[2019 Lopez - Fine-Grained Entity Typing in Hyperbolic Space|2019 Lopez]], [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|2019 Lin]]. [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|Karn et al.]] applied encoder-decoder attention mechanism [134] on their Bi-GRU context encoder. [[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds|2018 Zhang]] used an L-layer stacked LSTM with mention attention, a dot-product attention [135] computed based on the alignment between the entity and its context. [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention|Xin et al.]] explored three kinds of attention on their Bi-LSTM: self-attention, mention attention and knowledge attention, a dot-product attention computed using entity embedding learned from an external KB. 
[[2018 Zhang - Path-Based Attention Neural Model for Fine-Grained Entity Typing|2018b Zhang]] proposed path-based attention model. The path-based attention for each type is computed on its path to its parent type on the type taxonomy. Such attention mechanisms improved the discriminativeness of attention weights, but still focus on local context.

**Document-level Context Encoder** 
[[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds|2018 Zhang]] proposed a document-level context encoder based on distributed memory model [137] and MLP. $v_d = ReLu (W_{d1}tanh (W_{d2}DM(x_d)))$ (3) where $DM$ is a pre-trained distributed memory model which converts a sequence of words of a document into a vector. $W$ are the weight matrices. 

**Pre-trained Contextualized Encoders** 
Using pre-trained contextualized encoders [138] has achieved significant improvements on almost all NLP tasks. Most of the pre-trained contextualized encoders are based on the Transformer [139] architecture. Notably, ELMo [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|2019 Lin]],  BERT [140], [141], [142], [[2020 Zhang - MZET, Memory Augmented Zero-Shot Fine-grained Named Entity Typing|2020 Zhang]], [[2021 Onoe - Modeling Fine-Grained Entity Types with Box Embeddings|2021 Onoe]], SpanBERT [145], [[2021 Hou - Transfer learning for fine-grained entity typing|2021 Hou]], RoBERTa [147], [142], XLNet [148], [142] have been employed to encode contexts for FGET.

#### Entity Embeddings

Entity-level FGET makes predictions either based on a bag of mentions [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]], entity graphs [[2019 Jin - Fine-grained entity typing via hierarchical multi graph convolutional networks|2019 Jin]] , or based solely on learned entity embedding.

**Corpus-level Entity Encoder** 
[[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing|Yaghoobzadeh and Schutze]] treated the mentions of the same entity as a special word, and learned a corpus-level representation of that entity through a Structured SkipGram [121] (SSKIP [152]) which incorporates order of words into learning objectives. The entity embeddings learned in this way can only capture a short context of each mention.

**Attributed and Predictive Entity Embeddings** 
[[2019 Jin - Fine-grained entity typing via hierarchical multi graph convolutional networks|Jin et al.]] employed a standard feed forward neural network to learn entity embeddings. The input is the adjacency vector (a binary vector represents whether two entities in a KB have relations) and the attribute vector of the entity. The neural network was trained with multi-tasks: (i) predict the neighbouring entities; (ii) predict the semantic types of typed entities. The learned entity embedding can be used to predict types of entities without type tags. But this model can only learn the embeddings of in-knowledge-base entities.

#### Type (Label) Embeddings

Embedding feature representations and type labels into a common dimensional space  allows for information sharing between related types. Such label embeddings can make typing models robust to label noises and types that are not present in the training data. [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [153], [[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding|2016a Ren]], [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing|2016 Ma]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]], [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]], [132], [[2019 Jin - Fine-grained entity typing via hierarchical multi graph convolutional networks|2019 Jin]], [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|2019 Lin]], [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|2018 Obeidat]], [[2019 Lopez - Fine-Grained Entity Typing in Hyperbolic Space|2019 Lopez]], [141] .
WSABIE [154] based approaches [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding|2016a Ren]], [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing|2016 Ma]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]], [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]] learn two mapping matrices to project the hand-crafted feature vectors and one-hot binary type vectors into a common low-dimensional space respectively. The matrices are learned using the weighted approximate pairwise (WARP) loss [154], a ranking loss that encourage placing positive types above negative types. Such approaches still rely on NLP tools to extract handcrafted features, and a large number of parameters in the two matrices need to be learned. The average of the embeddings of the words comprising the type name was used as type embeddings [[2018 Zuan - Otyper, A neural architecture for open named entity typing|2018 Zuan]]. Directly learning entity embeddings and type embeddings in a common Euclidean space [[2019 Jin - Fine-grained entity typing via hierarchical multi graph convolutional networks|2019 Jin]], [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|2019 Obeidat]], [141] can obviate the reliance on NLP tools and reduce the number of parameters. But the Euclidean space is not good at capturing type correlations or hierarchies. Thus, some approaches [[2019 Lopez - Fine-Grained Entity Typing in Hyperbolic Space|2019 Lopez]], [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|2019 Lin]] propose to project feature and type vectors onto a hyperbolic space to capture the underlying type hierarchical information. Box embeddings, which represent types and mentions as boxes [[2021 Onoe - Modeling Fine-Grained Entity Types with Box Embeddings|2021 Onoe]], can capture hierarchies of types even when these relationships are not defined explicitly in the taxonomy.

#### Strategies for Learning Feature Embeddings 

The following two strategies have been widely employed to learn effective feature embeddings for FGET. 

**Loss Functions** 
Hinge loss function is used to maximize the scoring margin between positive examples and negative examples [153]. But the definition of negative examples is not trivial. Moreover, both the hinge loss and the commonly used (binary) cross-entropy loss function [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|2017 Karn]], [[2017 Yaghoobzadeh - Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities|2017b Yaghoobzadeh]], [[2017 Yaghoobzadeh - Noise Mitigation for Neural Entity Typing and Relation Extraction|2017c Yaghoobzadeh]], [[2018 Choi - Ultra-Fine Entity Typing|2018 Choi]], [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention|2018 Xin]] assume the types to be independent. Thus, type hierarchy loss functions are proposed to incorporate the hierarchical correlations among types, such as **(i)** the binary cross-entropy loss based on a score of a hypernym link between types [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]], [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017 Murty]]; **(ii)** the type order loss function for making the correct type get a higher score than its ancestor types and sibling types [[2018 Jin - Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases|2018 Jin]]; **(iii)** the hierarchical loss normalization process for penalizing less in the case where types are related [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu]]. 

**Multi-task Objective** 
As we noted in Section 1, FGET is highly related with other entity analysis tasks. Multi-task learning for FGET [[2017 Yaghoobzadeh - Noise Mitigation for Neural Entity Typing and Relation Extraction|2017c Yaghoobzadeh]], [[2018 Jin - Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases|2018 Jin]], [[2018 Choi - Ultra-Fine Entity Typing|2018 Choi]] can encourage the model to embed more information. For example, [[2017 Yaghoobzadeh - Noise Mitigation for Neural Entity Typing and Relation Extraction|Yaghoobzadeh et al.]] proposed a joint model for FGET and relation extraction. [[2018 Jin - Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases|Jin et al.]] used the same neural networks to jointly predict the neighbouring entities and semantic types of an entity in a KB, [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]] organized type labels into three bins (coarse-grained, fine-grained and ultra-fine), with the training objective set to minimize the logistic loss at three granularity.

#### Hybrid Feature Representations 

Most works on FGET have used either the hand-crafted sparse features or learned dense features. While [[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification|Shimaoka et al.]] considered the combination of hand-crafted features and embedded features with an attentive neural model. The sparse features are projected into a low-dimensional vector by a parametric matrix, then the projected vectors are used with mention embeddings and context embeddings. The experiment results show that the hybrid feature representation outperforms embedding-only features.

## TYPING METHODS 
In this section, we review the main techniques used to predict the types of entities (mentions), based on how the problem is modelled. In Section 5.1, we review the unsupervised methods. In Section 5.2, we inspect the semisupervised typing methods. In Section 5.3 - 5.5, we review the supervised methods of three categories: the independent model, the collective model and the joint model.

#### Unsupervised Approaches 
Unsupervised approaches [50], [31], [2] heavily rely on the lexicon-syntactic patterns to extract lists of entities of some category, e.g., list of cities, films. Pas¸ca [50] used manually selected lexicon-syntactic patterns (e.g., `<[Start Of Sentence] X [such as|including] N [and|,|.]>`) to extract pairs of type information and entity $(X, N)$. For example, the underlined parts of *“That is because software firewalls, including Zone Alarm, offer some semblance of this feature”* will be extracted as $X$ and $N$, respectively. The fine-grained types were obtained by selecting the rightmost non-recursive noun phrase whose last component is a plural-form noun, e.g., *software firewalls* is selected as the type of *Zone Alarm*. Considering the low recall of such patterns, this method relies on millions of web documents to extract a comparably small list of entities. Etzioni et al. [2] proposed improving the recall of the syntactic patterns by learning high-quality patterns. They extracted candidate patterns from web search results of seed names. Each pattern was assessed with precision and recall, computed based on the statistics from search results. But this method relies on a search engine and is limited to simple text patterns.
Elsner et al. [155] proposed an unsupervised generative model for clustering named entities that incorporate named entity internal structure, its syntactic context and coreference information from an unsupervised pronoun resolver. A non-parametric Bayesian inference over context-free grammars is employed to find the types. However, this model is extremely prone to fall into local minima during inference because of the complexity of the model and needs to tune hyperparameters.

#### Semi-supervised Approaches 
![[2022_wang_table_4.png]]

**Corpus-based Entity Set Expansion** 

This approach iteratively extracts new typed entities from a corpus by exploiting the similarities between candidate mentions and the typed seed entities. This approach can only extract entities similar to seed entities from a corpus. Thus, it is not applicable for on-site typing. 
**Using Contextual Similarity** Cimiano and Volker [156] represented types and entities as context vectors using vector space model (VSM). The similarities in vector space were used to find the types of entities. The ontology population method [157] consists of five steps: **(i)** Collect N snippets containing mention $i$; **(ii)** Derive a list of hypothesis phrases by replacing $i$ with each seed entity $j$; **(iii)** Calculate the plausibility score $s_j$ using a scoring function; **(iv)** Obtain an overall score $s_c$ for type $c$ by summing the scores of all hypothesis phrases of type $c$; **(v)** Type the mention $i$ with the type having maximum score. Pantel et al. [158] applied a matrix of terms (words and entity mentions) to ESE using the set expansion algorithm of [159]. Each term is represented by a SVM vector of numerical features vi learnt using MapReduce on 200 billion words corpus. For a given set $S$ of seed entities of a type, the centroid of $S$ is a weighted average of its element entities. Then the similarity between seed set centroid and candidate mention was used to find the type.

The performance of these methods is heavily influenced by the size and quality of seed entities and corpus. The similarity is computed based on the sparse feature vectors using a vector space model. **This makes these methods perform badly in discriminating some similar fine-grained types, such as `actor` and `director`.** Yan et al. [160] proposed a novel Bootstrapping method combining the Monte Carlo Tree Search (MCTS) algorithm with a deep similarity network to address the sparse feature.

**Using Similarity of Syntactic Patterns** Pasca [161] extracted typed entities from the Web search queries using seed entities. Each query that contains a seed entity generates a query (syntactic) template. These template queries were used to extract other candidate entities. Each extracted candidate was represented as a binary search-signature vector. The search-signature of a type was generated by combining all seed entities vector. The similarity score between a candidate entity vector and a type vector was used to determine the type. **But the performance of this method is hindered by the noises in the search queries**. Carlson et al. [23] proposed a method that starts with a type taxonomy, a set of relations, a handful of seed entities, and a set of coupling constraints. During each iteration of Bootstrapping, four steps are taken: **(i)** Extract new candidate mentions and contextual patterns using recently promoted patterns; **(ii)** Remove candidate patterns that violate constraints; **(iii)** Rank those candidate patterns; **(iv)** Promote top patterns. But **this method suffers from the sparsity of patterns, and only considers the *Company* and *Sport* domain**.
To alleviate the context sparsity issue, ClustType [64] proposed the soft clustering of relation phrases that signifies a unary or binary relation between two entity mentions. They used a phrase mining [162] algorithm on POS-tagged corpus to get candidate mentions and relation phrases. **But their seed entities were generated by an entity linking tool**. **Thus, the errors of the entity linking tool were inevitably propagated into the typing phase.**

**KB-based Candidate Types Ranking** This approach can perform on-site typing. For each untyped entity mention, this approach first retrieves a collection of related entities from a KB and generates some candidate types from the context and related entities. Then the candidate types are ranked according to the context. Vallet and Zaragoza [163] and Balog and Neumayer [51] used a collection of Wikipedia entities (with types) as supervision for ranking the types of entities. The former method [163] consists of four steps: **(i)** Execute the query using an information retrieval (IR) module to retrieve the 500 most relevant passages; **(ii)** Collect all entities in the retrieved passages; **(iii)** Rank these entities based on Kullback-Leibler distance; **(iv)** Rank the types for the query based on the ranking of those entities and their types. The latter method [51] represents each type as a pseudodocument, which contains the descriptions of all entities of that type. The types are ranked by an IR algorithm that execute the entity query on the corresponding documents. Both methods rely on an IR module, and the performance on entities having a large number of candidate types is poor. Instead of using an IR module to retrieve related entities, [[2018 Zhou - Zero-Shot Open Entity Typing as Type-Compatible Grounding|Zhou et al.]] used a pre-computed word-concept(entity) map and mention-entity prior to retrieve the most relevant concepts(entities) for an entity mention. The word-concept map, learned from the Wikilinks [92] corpus, is a collection of word representations, where each word is represented with a vector of Wikipedia concepts (entities).

To reduce the number of candidate types, [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|FINET]] and SANE [165], [166] decomposed the ranking into two steps: candidate types extraction and type selection. They extracted candidate types (restricted to hyponyms of coarse-grained type) from context using Hearst patterns [31], Appositional, Copular patterns, etc. To select the most appropriate types that best fit the context, FINET [25] trained a Naive Bayes classifier per coarse-grained type using automatically generated data from WordNet. This classifier works like WSD using WordNet. However, the performance of this model is hindered by the NER tool, which may generate false coarse-grained types.

**Type Propagation on Graphs** 
This approach first extracts information from corpora and build graphs to represent the relations between typed (or linked) entities, untyped entities and type labels. Then different algorithms are used to propagate type labels from typed entities to untyped mentions. Talukdar et al. [167] used graph model to represent type-instance relations, extracted from unstructured text or structured HTML tables. Kozareva et al. [168] found that the instance-instance graph, whose edges represent the neighbouring appearance in the doubly-anchored patterns (DAP, e.g., `〈semantic-type〉` such as `〈seed〉` and `∗`), was more effective than type-instance graph for type propagation. [[2019 Lin - An Attentive Fine-Grained Entity Typing Model with Latent Type Representation|Lin et al.]] exploited the textual relations. For example, the typed *Microsoft* and *Google* occur with textual relations such as *"has already announced"* and *"has released updates for"*, and the untyped *Sun Microsystems* also appear with similar textual relations. The most frequent Freebase types of similar entities were extracted as the type for *Sun Microsystems*. Such type propagation methods inevitably suffer from sparsity as they rely on the textual phrases.

**The above methods do not consider assigning multiple compatible types to an entity.** PEARL [59] finds the most appropriate set of types for an entity using an integer linear program (ILP) under type disjointness or type correlation constraints. To overcome the sparsity, it allows for fuzzy matches of patterns by introducing the type-pattern likelihood, which is the likelihood of relational patterns p (i.e., the PATTY collection [169] of 300,000 typed relational paraphrases) occurring with entities of types $t_1, t_2$. **But this method performs poorly on the infrequent entities, since it does not have sufficient evidence for reliable type assignment**.

Another method that can propagate multiple compatible types to entity mentions is the so-called universal schema for FGET [[2013 Yao - Universal Schema for Entity Type Prediction|201 Yao]]. It embeds entities and types by maximizing the log likelihood of the observed units of an entity-type matrix. The missing units, i.e., the missing types of an entity, are predicted by matrix completion. However, **it can only extend the type set of an entity by using semantic implications between types learned from the observed units of the matrix. The ambiguities in the 16k flat types are ignored.**

**The above methods can be viewed as entity-level FGET**, and they ignore the label noises in the seed entities. [[2016 Ren - AFET, Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding|AFET]], a mention-level FGET method, reduces the label noises by separating the loss functions for clean and noisy seed examples. It embeds hand-crafted features and type correlation in a common dimensional space. In addition to the text features and type labels, CoType [94] also embeds the relation phrases and relation types into the common space. It selects types by computing the cosine similarity between entity (relation) mention embeddings and type label embeddings. **But the loss function parameters for both methods need to be tuned on a different corpus.**

#### Supervised Approaches: Independent Models 

Most of the supervised typing models apply a classifier on each mention independently. Such independent models do not consider the type relatedness between neighbouring mentions in the same document. **Most methods for mention-level FGET are of this category**. Entity-level FGET methods apply three variants [82], [28], [36], [171], [88] : 
	(i) Use the same classifier with entity-level feature representations of the entity as input; 
	(ii) Treat each resolved mention as mention-level FGET, then combine the types of the same entity in different contexts; 
	(iii) Combine the predictions of (i) and (ii). FGET is a hierarchical classification problem. According to the categorization of hierarchical classifiers by Silla and Freitas [172], there are mainly four categories of hierarchical classifiers used for FGET

- Flat: using a single multi-class classifier for all types. 
- Local: using a binary classifier for each type, enforcing label consistency at inference time. 
- Local per Parent Node: using a multi-class classifier for each parent type node; the target classes are the children types. 
- Global: using a single multi-class classifier trained with a loss function that considers label similarity

The characteristics of these classifiers are summarized in Table 5.
![[2022_wang_table_5.png]]

**Flat Classifiers** 
Flat classifiers ignore the type hierarchy and treat FGET as a multi-class classification problem. For example, [[2002 Fleischman - Fine Grained Classification of Named Entities|Fleischman and Hovy]] employed a decision tree model trained with C4.5 algorithm [173] to classify person names into 8 types. [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] treated the mention-level FGET as a multi-class multi-label classification, and employed a classic linear classifier, Perceptron [174] as their flat classifier. $\hat t = arg max_t w^T · f(x, t)$ where $\hat t$ is a predicted type, $f(x, t)$ is the sparse feature vector of a mention $m$ with a type $t \in T$, and $w$ is the weight vectors of the feature functions. **Apparently, such a classifier does not consider the compatibility between multiple types assigned to the same entity**. The skewness of the corpus (217 of the 562 entities in the test data are labelled with only `Person`) makes this model **sensitive to label noises in training examples**.

Similar linear classifiers for FGET [[2015 Neelakantan - Inferring Missing Entity Type Instances for Knowledge Base Completion, New Dataset and Methods|2015 Neelakantan]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing|2016 Ma]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]] use the dot product between feature embeddings and label embeddings. The inference strategy is different, they either select the top-k scored types and greedily remove inconsistent types (i.e., not on the same type path) [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [[2016 Ma - Label Embedding for Zero-shot Fine-grained Named Entity Typing|2016 Ma]] or recursively select the type with highest score among siblings till a leaf type is encountered [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]]. It should be noted that mapping hand-crafted feature vectors into low-dimensional space may cause the training to converge only to a local optimum solution because of the non-convexity of the loss functions [[2015 Neelakantan - Inferring Missing Entity Type Instances for Knowledge Base Completion, New Dataset and Methods|2015 Neelakantan]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]].

Another approach uses softmax classifier or multi-class maximum entropy model [175] as flat classifier for FGET [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]], [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]], [[2018 Jin - Attributed and Predictive Entity Embedding for Fine-Grained Entity Typing in Knowledge Bases|2018 Jin]]. This approach usually transforms a multilabel training example into examples for multi-class classification, i.e., convert a training example with multiple labels into multiple examples with a single type label. For example, mention *”Canada”* with coarse-grained types `location` and `organization` are transformed into two training examples, one with fine-grained type of `location` and the other with type of `organization`. The inference strategy is selecting all types whose probability exceeds a threshold [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]]. However, the experimental results show that the predictions on top-level types are more reliable than that on lowerlevel types [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]]. This indicates the performance of flat classifiers is sensitive to the number of types.

**Local Classifiers** 
Local classifiers make binary local predictions for each type independently. The local classifiers for FGET include: binary Support Vector Machines (SVM) [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|2012 Yosef]], [75], maximum entropy classifier [175] or logistic regression classifier [97], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]], [[2016 Suzuki - Fine-grained named entity classification with wikipedia article vectors|2016 Suzuki]], bi-linear score [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|2019 Obeidat]], sigmoid Multi-Layer Perceptrons (MLP) [[2015 Yaghoobzadeh - Corpus-level Fine-grained Entity Typing Using Contextual Information|2015 Yaghoobzadeh]], [[2016 Shimaoka - An Attentive Neural Architecture for Fine-grained Entity Type Classification|2016 Shimaoka]], [[2016 Jia - Fine-grained Entity Typing for Knowledge Base Completion|2016 Jia]], [[2017 Shimaoka - Neural Architectures for Fine-grained Entity Type Classification|2017 Shimaoka]], [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017 Murty]], [[2017 Yaghoobzadeh - Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities|2017 Yagoobzadeh]], [[2018 Zhang - Path-Based Attention Neural Model for Fine-Grained Entity Typing|2018b Zhang]], [[2018 Zhang - Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds|2018 Zhang]], [[2018 Choi - Ultra-Fine Entity Typing|2018 Choi]], [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018 Murty]], [[2018 Xin - Improving Neural Fine-Grained Entity Typing with Knowledge Attention|2018 Xin]], [[2015 Yaghoobzadeh - Corpus-level Fine-grained Entity Typing Using Contextual Information|2015 Yaghoobzadeh]], [[2018 Yuan - OTyper , A Neural Architecture for Open Named Entity Typing|2018 Yuan]], [[2019 Dai - Improving Fine-grained Entity Typing with Entity Linking|2019 Dai]], [[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing|2019 Xiong]].

According to the survey of Silla and Freitas [172], the local classifiers usually perform better than flat classifiers. [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] showed that local classifiers outperformed flat classifiers based on their empirical settings. However, **defining positive and negative training examples for local binary classifiers is not trivial**. Moreover, **inference strategies need to be devised to enforce label consistency** (e.g., `person` and `artist` are consistent, while `location` and `artist` are not consistent) as the local classifiers make predictions independently. The positive examples for type t are generally from the examples labelled with t or its descendants in the taxonomy [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]], [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|2012 Yosef]], [97]. For example, a mention labelled `person/political` figure is considered a positive example for `person`. Negative examples are defined in three ways: **(i)** Examples of all the other sibling types; **(ii)** Examples of all the other types at the same level; **(iii)** Examples of all the other types. [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|HYENA]], [75] used negative examples **(i)** to train their binary SVM classifier. A child type `Others` was added to each of the non-leaf types, since the child types of a parent type do not necessarily cover all possible subtypes. Positive samples for Others are instances of type t that do not belong to any of its sibling types. When [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] experimented with negative examples (i)-(iii), they found that negative examples (ii) was the most effective way for training local classifiers.

Different inference strategies are devised to enforce the label consistency between local classifiers. [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] considered three inference strategies: **(i)** Independently assign all types whose probability exceeds the threshold, which may lead to multiple type paths or no path on a taxonomy; **(ii)** Multiply the probabilities on a type path, which ensures the parent type is also assigned with child type; **(iii)** Compute the marginalization of a type using the sum-product algorithm on all possible type paths, and then employ strategy (i). Their experimental results showed that strategy (iii) is the best. Considering the fact that the prediction threshold for each SVM can be highly type-dependent, [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|HYENA]], [75] trained a meta-classifier to select the optimal number of top-n types for each mention during inference. Ekbal et al. [97] experimented with level-wise strategy and global strategy. The level-wise strategy selected the type with the highest probability among types of the same level. The global strategy was similar to the strategy (ii) of [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]]. Their experimental results preferred the level-wise strategy.

**Local per Parent Node** 
Local per Parent Node (LPPN) approach makes top-down predictions. Starts from the virtual root of a type taxonomy tree, LPPN implements a multi-class classifier for a parent type to determine which child type should be assigned to the mention. The assigned child type is then treated as a new parent type and a multi-class classifier is implemented for it. This process continues until a leaf type is assigned to the mention. Thus, **LLPN always generates consistent type labels**. Compared with binary local classifiers, LPPN only implements multi-class classifiers for parent types and a virtual root type. [[2017 Karn - End-to-end trainable attentive decoder for hierarchical entity classification|Karn et al.]] proposed an encoder-decoder architecture for the mention-level FGET. The averaged mention embedding was used to initialize the state $s_0$ of the decoder, a standard RNN with attentive mechanism whose output at each step represents a node on the predicted type path. At each step, the attentive weight of each context vector was updated based on the state vector of the last step, and the output was computed using a MLP with element-wise sigmoid: $t_i = \sigma(s_{i−1}, c_i)$. The next state was computed as $s_i = f(s_{i−1}, t_i , c_i)$. For example, the output at each step could be $t_0=$ `<SOL>` (Start of Label), $t_1=$`Person`, $t_2 =$ `political_figure`. The advantage of this classifier is that **it always predicts consistent types (types on a type path)**. The disadvantage is that **the errors at the first step may cause all the types to be wrong**.

**Global Classifiers** 
Using global classifiers, the dependencies and hierarchical relations between different types (e.g., any example belonging to `person/artist` automatically belongs to `person`) can be taken into account [172].

The intuition behind a global model is that a **parent type of the true type is more preferable to the other unrelated types**. For example, if a mention’s true type is `person/artist/actor`, it is better to predict its type as `person/artist` than `location`, `organization` or their child types. To encourage this preference, the loss function should be modified to penalise the scenario less when predicted types and true types are related. [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|Xu and Barbosa]] introduced a process called hierarchical loss normalization, which computes the estimated probability as follows. $p^* (\hat t | m, c) = \hat p ( \hat t | m, c) + \beta * \sum_{t \in \Gamma} \hat p(t | m, c)$ where $\Gamma$ is the set of ancestor types along the type-path of $\hat t$, $\beta$ is a hyperparameter. This value then is re-normalized into a real probability distribution for maximizing the likelihood of a corpus.

Given a featurizer $\phi$ that takes as input a sentence $x$ and entity $e$, [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments|Rabinovich and Klein]] found a set of types $T^*$ using a linear model. The feature functions consider the compatibility of types as follows: 
![[2022_wang_eq_6.png]]

**The disadvantage of global models is that they are computationally more expensive**

#### Supervised Methods: Collective Models

**One weakness of independent models is being unable to take into account correlations between entity mentions in a document. Collective models for FGET type the mentions in a document simultaneously through incorporating relational information**

[[2010 Rahman - Inducing Fine-Grained Semantic Classes via Hierarchical and Collective Classification|Rahman and Ng]] proposed a collective model that exploited the relationship between two mentions by introducing a pairwise factor node in a factor graph [176], where each variable node represented a mention in a document. The pairwise relationship between two mentions was determined by a heuristic coreference resolution [14] process. If two variable nodes are determined to be coreferent, a factor node will connect both nodes, and the compatibility function of the pairwise factor encourages the assignment of the same labels to the two variable nodes. They incorporated hierarchical information of type taxonomy into the definition of compatibility function as follows: 
![[2022_wang_eq_7.png]]

where $sup(t_p)$ is the parent type of $t_p$ in the hierarchical taxonomy, and $P_{sup}(sup(t_p) | m_i)$ is the probability that mention $m_i$ is type $sup(t_p)$ according to the classification model $P_{sup}$, a Maximum Entropy model determining the coarse-grained types of entity mentions. However, this collective model only considered the coreference relations between entity mentions of a document.

#### Supervised Methods: Joint Models 
As we mentioned, the tasks of entity analysis are highly interdependent. Joint models tackle FGET and other tasks jointly to capture more information for making globally optimized decisions [20], [18], [16], [19]. These models are mainly based on a factor graph model [16], [19], and their experimental results show that joint models improve performance on all the tasks incorporated. Pantel et al. [177] proposed a graph based generative model to jointly model user intent and query entity types. The model was trained by maximizing the probability of observing a large collection of real-world queries and their clicked hosts. This method could only type the entities that appear in their web queries. Singh et al. [19] used the factor graph model to represent the dependencies between entity typing, relation extraction and coreference resolution. Instead of training all the factors jointly, they used a piece-wise training approach to estimate the parameters of the model. Parameters for each factor were learned independently by maximizing the piece-wise likelihood. Durrett and Klein [16] tackled coreference resolution, entity typing and entity linking simultaneously using a conditional random fields (CRF) model. Unary factors in the CRF model defined the features for solving each task independently. The binary and ternary factors defined the features that captured the interactions or constraints between tasks. The model was trained by maximizing the joint probability of three labels for all mentions in the corpus. However, for both learning and decoding, exact inference would be intractable because of the loops in the factor graph. Although belief propagation can perform efficient inference, it would still be computationally exorbitant due to the ternary factor. Thus, they used a pre-trained coarse model to prune 90% of the possible coreference arcs. However, this joint model requires a training corpus that has labels of all three tasks, and this is not readily available.

## TAIL TYPES AND NEW ENTITIES 
In this section, we review the special considerations for tail types and new entities. Tail types, which form the long tail of distributions of types in a corpus or KB, are those semantic types that have few or even no training examples in a corpus. It was noted that the performance of FGET models on a type is highly dependent on the number of training instances for that type [[2015 Neelakantan - Inferring Missing Entity Type Instances for Knowledge Base Completion, New Dataset and Methods|2015 Neelakantan]], [168]. The performance of [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|HYENA]] on the 5 top-level types is higher than on the tail types. [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]]  observed that their typing model often preferred coarse-grained types instead of ultra-fine-grained types, because many ultra-fine-grained types have few instances. Based on the WSABIE [154] method, a joint representation of features and type labels can improve performance on tail types [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|2015 Yogatama]], [[2017 Abishek - Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings|2017 Abishek]]. Using Glove embeddings directly as type embeddings [[2018 Yuan - OTyper , A Neural Architecture for Open Named Entity Typing|2018 Yuan]] is simple but effective for tail types. Pre-trained label embeddings that encode semantic relations and dependency between types [[2016 Ma - Zero-shot dataset partition|2016 Ma]], [[2019 Xiong - Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing|2019 Xiong]], [[2020 Zhang - MZET, Memory Augmented Zero-Shot Fine-grained Named Entity Typing|2020 Zhang]], label embeddings in box space [[2021 Onoe - Modeling Fine-Grained Entity Types with Box Embeddings|2021 Onoe]] and label embeddings from Wikipedia [[2019 Obeidat - Description-Based Zero-shot Fine-Grained Entity Typing|2019 Obeidat]] or label components embeddings [[2020 Kato - Embeddings of Label Components for Sequence Labeling, A Case Study of Fine-grained Named Entity Recognition|2020 Kato]] are proven to be effective for tail types.

New entities are those neither included in nor linked to a KB. It is noted that entity linking systems perform poor on those new entities [[2012 Ling - Fine-Grained Entity Recognition|2012 Ling]], thus the entity-level FGET systems are unlikely to handle those new entities competently. One category of methods is using linked entities as distant supervision for typing the new entities [45], [169]. They both leverage the relation phrase patterns, extracted from linked entities with types, to predict the types of new entities. Another category of methods utilizes the compositional nature of entity mentions to improve performance on uncommon (unseen) entity mentions [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]], [[2017 Yaghoobzadeh - Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities|2017 Yaghoobzadeh]]. It has been also found that fuzzy linking an entity mention to a collection of Wikipedia entities is effective for new entities and tail types [[2018 Zhou - Zero-Shot Open Entity Typing as Type-Compatible Grounding|2018 Zhou]].

The performance of FIGMENT [82] on tail types and new entities is poor, but the authors proposed the following suggestions for improvement: (i) Training on larger corpora; (ii) Refining type taxonomy to cover more entities; (iii) Exploiting hierarchical relations of types; (iv) Exploring more effective feature representations. Multi-level representations [[2017 Yaghoobzadeh - Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities|2017 Yaghoobzadeh]] include entity-level, word-level, subword-level, characterlevel representations, etc. The performance of multi-level representations on tail types is more than 6% higher than standard entity-level representations. The performance on tail entities is improved as well.

## SUMMARY AND FUTURE DIRECTIONS 
In this paper, we present a comprehensive and extensive review of fine-grained entity typing in the following aspects: the type taxonomy, the methods for automatically labeling corpus, the representation of features, typing models, methods for tail types and evaluation metrics. How to efficiently embed features and reduce the negative impact of label noises and skewed training examples is challenging. Embedded features and deep neural networks have been proven to be an effective solution for discerning those label noises. There is a paucity of research on the semantic type relatedness between entity mentions in a document. The document-level entity relatedness has been exploited in entity linking and proved to be effective [183], [184]. Although there has been plenty of research on FGET as presented in this review, we believe that more work still needs to be done to substantially improve the performance of FGET for real world applications. We propose the following research directions for FGET. Firstly, we can employ more effective feature encoders to learn feature embeddings. Most neural network based FGET systems employ shallow neural networks (such as CNN or BiLSTM) to learn the representations of mentions and contexts. We believe the Wikipedia pre-trained language model [185] that encode knowledge into Transformer model parameters can be more effective for feature embedding. Secondly, we can incorporate the document-level relatedness between semantic types into a typing model to make collective decisions, i.e., a collective model for FGET [11]. The global model for entity linking [186], [183], [184], which links the entity mentions collectively by considering the coherence between the referenced entities in a document, has been proven to be effective for improving performance. A deep learning based collective model for FGET can be a promising direction. Thirdly, a joint model for FGET and other entity analysis tasks is also a promising research direction, e.g., jointly tackle the entity linking and mention typing tasks with a unified model. Actually, plenty of research on joint models for entity analysis tasks has been conducted [16], [23], [24], [94], but most of current models are solely based on factor graph models with hand-crafted features. A multi-task neural network for FGET and other entity analysis tasks may learn more sufficient feature representations and capture the relatedness between these tasks.



---
Ruili Wang, 
Feng Hou, 
Steven F. Cahan, 
Li Chen, 
Xiaoyun Jia,
Wanting Ji

#paper 