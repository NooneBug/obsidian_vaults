https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9706297

### Abstract  
Fine-grained entity typing (FGET) is an important natural language processing (NLP) task. It is to assign fine-grained semantic types of a type taxonomy (e.g., `Person/artist/actor`) to entity mentions. Fine-grained entity semantic types have been successfully applied in many natural language processing applications, such as relation extraction, entity linking and question answering. The key challenge for FGET is how to deal with label noises that disperse in corpora since the corpora are normally automatically annotated. Various type taxonomies, typing methods and representation learning approaches for FGET have been proposed and developed in the past two decades. **This paper systematically categorizes and reviews these various typing methods and representation learning approaches to provide a reference for future studies on FGET. We also present a comprehensive review of type taxonomies, resources, applications for FGET and methods for automatically generating FGET training corpora. Furthermore, we identify the current trends in FGET research and discuss future research directions for FGET. To the best of our knowledge, this is the first comprehensive review of FGET.**

## Introduction
It is essential to recognize entities such as names (e.g., names of person, organization and location) and numeric expressions (e.g., time, date, money and percent expressions) in natural language text. Identifying references or mentions of these entities in text was acknowledged as an important natural language processing (NLP) task in the Sixth Message Understanding Conference (MUC-6) ([[1996 Grishman - Message Understanding Conference - 6, A Brief History|MUC-6, a brief history]]). Named Entity Recognition (NER) is to recognize and categorize named entities into coarse-grained classes (typically 4 classes: `Person, Location, Organization and Miscellaneous` ([[2003 Sang - Introduction to the CoNLL-2003 Shared Task, Language-Independent Named Entity Recognition|CoNNL]])). However, this kind of coarse-grained type information is insufficient for many applications, while fine-grained entity types can lead to substantial improvements on many NLP tasks ([[Downstream task in which ET is usefull]]). Thus, there has been a booming research stream on Fine-grained Entity Typing (FGET), which is to classify entity mentions into more fine-grained semantic types. For example, *Leonardo DiCaprio* is typed as `person/artist/actor`. The mentions of entities are complicated. A large proportion of entity mentions are coreferences (alias, nickname, pronoun, abbreviation, etc.) and ambiguous names. Furthermore, quite a number of entity mentions in textual documents are new entities that are not included in a knowledge base (KB). Thus, several tasks of entity analysis were developed to tackle these problems. For example, Entity Mention Detection (EMD) is to identify all the mentions of entities, including names, noun phrases and pronouns that refer to entities; Entity Coreference Resolution [14] is to identify and cluster entity mentions that refer to the same entity; Entity Linking is to resolve the ambiguity of an entity mention by linking it to a specific entity in a KB. All these tasks, along with FGET, are clustered as entity analysis. 
These entity analysis tasks are highly interdependent. Thus, an approach for FGET may tackle FGET with other tasks simutaneously [18], [19], [20], [21], [22], [23], [16], [24]. For example, Durrett and Klein [16] proposed a model that tackles multiple tasks jointly by taking advantage of cross-task interactions between FGET, entity coreference resolution and entity linking using a structured conditional random field (CRF).

**Motivations for this review:** Fine-grained type information of entities (mentions) have been successfully applied in many NLP tasks. FGET has attracted significant attention, and a considerable number of approaches with different type taxonomies have been proposed. However, there are no reviews on FGET approaches and related issues. To the best of our knowledge, **this is the first comprehensive review of FGET**

**Contributions of this review:** We systematically categorize, compare and analyze various typing methods, representation learning approaches. The other issues, such as KB resources, FGET applications, training corpora generation and type taxonomies, are also reviewed to provide useful resources for the FGET research community. Further, we identify the current trends in FGET research: (i) Learn embedded feature representations to address the challenges posed by label noises, tail types and new entities; (ii) Tackling FGET and other entity analysis tasks jointly is also a promising direction.

## Background

In this section, we first give a formal definition of the FGET task, and then review the applications of FGET as well as the KB resources for FGET.

##### Task Definition

Given a document $d$ or a collection of documents $D$ that contains a set of entity mentions $M$, and a predefined taxonomy of fine-grained semantic types $T$, the task of FGET is to assign each entity mention $m \in M$ an appropriate type $t \in T$ on the type taxonomy. An entity may have multiple types in different contexts, e.g., *Donald Trump* is a political figure, a businessman and an actor. FGET can be divided into two kinds of tasks: *mention-level FGET* and *entity-level FGET*. The formal definitions are given as follows:
- **Entity Mention:** Entity mention is a continuous span of tokens in the text which refers to a real world entity. Entity mention can be a named entity mention, a nominal mention or pronoun coreference.
- **Type Taxonomy:** Type taxonomy or type ontology is a tree or a directed acyclic graph (DAG) $O = (T, R)$, where $T$ is the set of semantic types and $R$ is the edge set. $R = {(t_i , t_j ) | t_i , t_j \in T, i \neq j}$ is also called the relation set, in which $(t_i , t_j )$ means that $t_j$ is a sub-type of $t_i$ .
- **Mention-level FGET:** Mention-level FGET can be defined as $f : M × C \rightarrow T$ ($C$ is the set of the corresponding context of each mention in $M$), which is to find a semantic type with the appropriate degree of granularity for an entity mention within a specific context. Mention-level FGET is also called context-dependent FGET.
- **Entity-level FGET:** Entity-level FGET can be defined as $f : M × D \rightarrow T$, which is to find all possible semantic types for an entity. The type set of an entity should be the union of mention-level FGET in different contexts. Entity-level FGET is also named context-independent FGET or corpus-level FGET. Entity-level FGET is mainly motivated for KB construction and completion.

An example of mention-level FGET is presented in Fig. 1. *Donald Trump* has multiple types, including political figure, business and TV show actor. But from the context given by the sentence, `Person/political figure` is the appropriate type path, so the type label could be `Person` or `political figure`.
![[2022_wang_fig_1.png]]

For entity mentions, some approaches [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification]], [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions]], [[2017 Yaghoobzadeh - Multi-Multi-View Learning, Multilingual and Multi-Representation Entity Typing]], especially those entity-level FGET methods, assume that the gold entity mentions are segmented by NER or EMD tools. Some methods view FGET as fine-grained NER, treat the mention detection as an integrated part of FGET, and organize the mention detection and entity typing in a pipeline way [[2012 Ling - Fine-Grained Entity Recognition]], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging]]. Some methods for automatically generating training data also work in this way. A detailed summary of mention detection in automatically creating training corpora is given in Section 3.2.1

Entity-level FGET is related to word clustering [30] (or many other names, e.g., semantic class identification, hyponym acquisition [31], semantic lexicon induction, semantic class learning [32]), which is to assign words to classes based on the statistics from large corpus. This task originates from the class-based n-gram language model [33], which reduces the problematic effects of sparsity by grouping similar words into classes. Mention-level FGET is similar to word sense disambiguation (WSD) [34]. WSD is a task of determining the fine-grained semantic class of words in contexts. Both mention-level FGET and WSD need to make decisions based on the contexts of mentions or words. **Compiling semantic type taxonomy is related to the automatic ontology construction, which aims to create lexical hierarchies based on semantic classes [35]. However, type taxonomies used for FGET were manually curated or semi-automatically mapped from type taxonomies of a KB.**

#### Type Taxonomy and Training Corpus

In this section, we present a review of how the type taxonomies are curated and how the training corpora are automatically generated.

![[2022_wang_taxonomies_table.png]]

**Type Taxonomies** 
FGET uses far more complex type taxonomies as opposed to traditional NER. There are two categories of type taxonomies: manually curated and semi-automatically curated. Manually Curated Type Taxonomies Initially, Locations were manually classified into 8 fine-grained subtypes (i.e., *Country, City, Street, Territory, Region, Water, Mountain*, and *Artifact*) [[2001 Fleischman - Automated Subcategorization of Named Entities]] and Persons were classified into 8 subcategories [[2002 Fleischman - Fine Grained Classification of Named Entities]] (i.e., *Athlete, Politician/Government, Clergy, Businessperson, Entertainer/Artist, Lawyer, Doctor/Scientist, Police*).

Sekine et al. [72] compiled a taxonomy of 150 types (extended to 200 types [73] later) with maximum depth 5 to make it broad enough for general applications. They used three methods (i.e., based on corpus, based on previous systems and tasks, and based on thesaurus) to design three initial taxonomies. Then they merged the three taxonomies into one and refine it. Lee et al. [42] introduced a set of 147 fine-grained types with two levels (the 15 top-level types include Person, Study Field, Theory, Artifacts, Organization, Location, Civilization.) for question answering

**Semi-automatically Created Type Taxonomies** 
KBs have been used for semi-automatically generating type taxonomies. For example, Wikipedia categories have been used to derive a taxonomy [86], [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments]]. Those categories whose syntactic heads cannot be found in WordNet [58] are removed, and types are obtained by mapping categories to their syntactic heads. Compared with the Sekine type taxonomy [73], Wikipedia category based type taxonomy is more like a semantic tag than a well-defined type taxonomy, while Freebase semantic types provide broader coverage of entities and types, and allow for multiple types being assigned to one entity. But the Freebase types are noisy since they are labelled by non-expert volunteers. Thus, [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] took two steps to get a well-defined type taxonomy [[Dataset - FIGER|FIGER]] from Freebase types: (i) Irrelevant types are filtered out, and only those types with more than 5 ground instances in Freebase were kept; (ii) Those too specific types were manually merged. Similarly, the GFT ([[Dataset - Ontonotes v5 - GFT]]) type taxonomy [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|2014 Gillick]] was obtained by first organizing the non-hierarchical types (the level 2 types) of FIGER into a hierarchy and then refining them by discarding rare or ambiguous types. New types were inserted if there were enough instances to corroborate the insertion. Although the coverage and specificity limitations of Freebase types may be exacerbated by such manually merging, FIGER and GFT are two of the most widely used taxonomies.

The semantic types of other KBs are leveraged to derive type taxonomy as well. For example, [[2012 Yosef - HYENA, Hierarchical Type Classification for Entity Names|Hyena]] types were derived from [[Dataset - YAGO505|505 YAGO]] types. The 22 top-level types of [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]] were derived from the ontology of DBpedia [65]. PEARL [[2013 Nakashole - Fine-grained semantic typing of emerging entities|2013 Nakashole]] used the YAGO2 [71] type taxonomy, which was derived from 200 WordNet classes.

The ultra-fine grained type taxonomies normally have thousands of types. For example, FINET [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|2015 Del Corro]] consists of more than 16K types with top categories Person, Location and Organization. [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]] organized types into three disjoint bins: 9 general types, 121 fine-grained types and 10,201 ultra-fine grained types. Murty et al. [[2017 Murty - Finer Grained Entity Typing with TypeNet|2017]] [[2018 Murty - Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking|2018]] manually mapped Freebase types to the WordNet hierarchy, and got a type taxonomy of 1941 types with average depth 7.8. The purpose of introducing ultra-fine-grained types is to improve the coverage and diversity of type taxonomies, and to alleviate the skewed label distributions of corpus which is heavily skewed toward coarse-grained types. However, the taxonomy with thousands of types also poses a great challenge to typing models. There are also domain-specific type ontologies. For example, Unified Medical Language System (UMLS) [89], a biomedical hierarchical concept ontology with average depth 14.4, contains over 3.5 million concepts. A summary of FGET type taxonomies is listed in Table 1. As we can see, extensive work has been done to curate type taxonomies for FGET. However, no agreement has been reached by the research community, as the granularity of semantic types is quite subjective and dependent on task needs.

### Training Corpora Generation

Manually annotating data with multiple fine-grained types is a daunting task. Thus, almost all the FGET training corpora are generated automatically, i.e., mentions are linked to a specific entity of a KB using an entity linking tool (such as Dbpedia Spotlight [90] or AIDA [91]) or anchor links in the Wikipedia, and they label the mentions with all types from the KB. As for the mentions, some mention-level FGET methods [26], [27], [28] and all methods of entity-level FGET treat mention boundaries as given ground truth. There exist publicly available labelled corpora, such as Wikilinks [92] and ClueWeb [93], in which entity mentions are identified and linked to a KB. In the following subsections, we review the issues involved in automatically generating training corpora.

**Entity Mention Detection** 
Some works on FGET, especially those on mention-level FGET, treat mention boundaries detection as an integrated part of automatically generating training examples. Nominal, pronominal, and named entity mentions are all considered as targets. FINET [[2015 Del Corro - FINET, Context-Aware Fine-Grained Named Entity Typing|2015 Del Corro]] directly uses Stanford CoreNLP tool to extract named entities with coarse-grained types. Thus, only the named entities are extracted, and the nominal phrases and pronouns are ignored. [[2013 Nakashole - Fine-grained semantic typing of emerging entities|Nakashole et al.]] proposed a method to detect and type noun phrases that refer to new entities. However, this method cannot be used to generate training examples directly, because it can only generate type labels in open vocabulary words and these labels need to be converted to type labels of a hierarchical taxonomy. To extract both named entity mentions and noun phrase, [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]] applied a POS3 tagger, a dependency parser and a noun phrase (NP) extractor in a pipeline way, where the noun phrase extractor relies on POS tags and dependency edges to detect entity mentions. But the pronominal mentions are ignored as there is no coreference resolution processing. [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] treated the segmentation of mentions as a sequence labelling task and trained a linear-chain CRF model on a heuristically-labeled Wikipedia data to detect entity mentions. Ren et al. [94] used quality mention examples from a KB as distant supervision and a random forest classifier to score segmentation quality. Many approaches [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]], [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [[2017 Rabinovich - Fine-Grained Entity Typing with High-Multiplicity Assignments|2017 Rabinovich]] use the anchor links in Wikipedia to detect entity mentions, but only less than 3% of text tokens in Wikipedia are anchored strings. Ghaddar and Langlais [61] proposed finding more non-anchored entity mentions by leveraging: (i) The out-link structure of Wikipedia, i.e., search the titles of the out-links in the current article; (ii) The coreferent mentions that refer to the main concept of a Wikipedia article extracted by WikiCoref [95], [96]. This can raise the coverage of mentions from less than 3% to about 30%.

**Corpora Automatic Labelling**
One approach for obtaining semantic types of a mention is using Wikipedia anchor links [60]. For each linked mention in a sentence, [[2012 Ling - Fine-Grained Entity Recognition|Ling and Weld]] and [[2015 Dong - A Hybrid Neural Model for Type Classification of Entity Mentions|2015 Dong]] found the corresponding Wikipedia entry via the anchor link; the semantic types were then mapped from Freebase or DBpedia to their own type taxonomy. WiFiNE [61], a large scale FGET corpus generated from Wikipedia, automatically maps the corresponding Freebase object type of each entity to GFT type taxonomy [29] or FIGER taxonomy [8]. The Wikipedia’s categories are not used since they are more like tags than a well-defined type taxonomy. The disadvantage of using anchor links is that the number of training examples is limited by the Wikipedia anchor links.

Another method of obtaining semantic types of a mention is using entity linking tools. [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [[2015 Yogatama - Embedding Methods for Fine Grained Entity Type Classification|Yogatama et al.]] and Ren et al. [94] used an entity linking system to retrieve Freebase types of resolved entity mentions, and then mapped Freebase types to their taxonomy. They used heuristics to reduce label noises. However, the linking errors can introduce label errors in the training examples. Moreover, both the Wikipedia link method and the entity linking tool method fail to label out-of-knowledge-base entities. The appositional structures are used to generate training examples of out-of-knowledge-base entities. Ekbal et al. [97] leveraged appositional patterns to extract person name-type pairs from ukWaC corpus. They used pattern frequency and WordNet information to reduce ambiguous and invalid semantic types. This method can achieve a 96.58% accuracy. But the number of examples is limited by the sparseness of apposition and the type is limited to Person

For [[2018 Choi - Ultra-Fine Entity Typing|ultra-fine labeling]], head words were directly used as type labels. However, head words labels are scarce and not context-sensitive. Thus, [[2021 Dai - Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model|Dai et al.]] proposed a method that combines hypernym extraction patterns [31] and a pretrained language model to generate ultra-fine labels .

**Label Noise Reduction**

The automatically generated type labels have two kinds of label noises: [[Noise Definition - Out-of-context|out-of-context]] noises and [[Noise Definition - Overly Specific|overly-specific]] noises [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss|2018 Xu et al.]]. To explain this, consider the mention Hugh Laurie in such two sentences: (i) "*Hugh Laurie* and his wife Jo Green were on the verge of divorce."" (ii) "*Hugh Laurie* wins Best Supporting Actor in a miniseries". *Hugh Laurie* has multiple labels (e.g., *actor, director, musician, comedian, and author*) in a KB, and all these labels will be assigned to both mentions. In Sentence (i), the type label should be *Person*, all other types are *overly specific* label noises (they are not out-of-context labels, since they are coherent with the context). In Sentence (ii), the type label should be `Person/artist/actor`, and all the other types are out-of-context label noises.

Using training examples with such label noises, some approaches ignore the noises and treat training corpus as normal [8], [26], [100], [101]. The performance of these approaches is decreased by such label noises. 

Another approach is to devise mechanisms to deal with the noises, to either directly tweak the noisy labels, such as [[2014 Gillick - Context-Dependent Fine-Grained Entity Type Tagging|Gillick et al.]], [77], [[2018 Choi - Ultra-Fine Entity Typing|Choi et al.]]; or enhance the typing model with the ability of tackling label noises, such as [102], [103], [104], [99]. These mechanisms are reviewed as follows:

---
Ruili Wang, 
Feng Hou, 
Steven F. Cahan, 
Li Chen, 
Xiaoyun Jia,
Wanting Ji

#paper 