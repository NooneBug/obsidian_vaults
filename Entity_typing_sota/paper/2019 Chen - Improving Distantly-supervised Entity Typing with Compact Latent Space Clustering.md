https://arxiv.org/pdf/1904.06475.pdf

*In this work, we propose to regularize distantly supervised models with Compact Latent Space Clustering (CLSC) to bypass confirmation bias problem and effectively utilize noisy data yet. Our proposed method first dynamically constructs a similarity graph of different entity mentions; infer the labels of noisy instances via label propagation. Based on the inferred labels, mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space, thus leading to better classification performance*

![[2019_chen_architecture.png]]

*our model encourages the feature extractor to group mentions of the same type as a compact cluster (dense region) in the representation space, which leads to better classification performance. For training data with noisy labels, instead of generating pseudo supervision by the typing model itself, we dynamically construct a similarity-weighted graph between clean and noisy mentions, and apply label propagation on the graph to help the formation of compact clusters*

*In contrast to PLL-based models, we do not force the model to fit pseudo supervision generated by itself, but only use noisy data as part of regularization for our feature extractor layer, thus avoiding bias accumulation.*

Use the [[Noise Definition - SIngle Path vs Multi-Path]] notion 

![[2019_chen_encoder_architecture.png]]

**Basic ideas**:

(1) all mentions belong to the same type should be close to each other in the representation space because they should have similar context, 
(2) similar contexts lead to the same type. 

For clean data, we compact the representation space of the same type to comply (1). 

For noisy data, given assumption (2), we infer the their type distributions via label propagation and candidate types constrain.

Clean data are used to train a classifier and feature extractor, noisy data is only used in CLSC regularization

Input Encoder is taken from [[2018 Xu - Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss]], so it is based on [[Encoders - Pretrained Word Embeddings]] (GloVe) concatenated with position embeddings.

- The mention encoder is an LSTM over the mention vectors
- The context encoder is a Bi-LSTM with attention over the entire sentence [[Encoders - Neural Based Models - Recurrent architectures]], [[Encoders - Neural Based Models - Attention based architectures]]

CLSC regularization is exhibited in Figure 4, which includes three steps: dynamic graph construction (Figure 4c), label propagation (Figure 4d, e) and Markov chains (Figure 4g). The idea of compact clustering for semisupervised learning is first proposed by (Kamnitsas et al., 2018). The basic idea is to encourage mentions of the same type to be clustered into a dense region in the embedding space. We introduce more details of CLSC for distantly supervised FET in following sections

![[2019_Chen_CLSC.png]]

The approach is evaluated on [[Dataset - Ren's Ontonotes]] and [[Dataset - Ren's BBN]]



#paper 