https://aclanthology.org/2021.insights-1.7.pdf

https://github.com/webersab/german_FET

#### Abstract

The training of NLP models often requires large amounts of labelled training data, which makes it difficult to expand existing models to new languages. While zero-shot cross-lingual transfer relies on multilingual word embeddings to apply a model trained on one language to another, Yarowsky and Ngai (2001) propose the method of annotation projection to generate training data without manual annotation. This method was successfully used for the tasks of named entity recognition and coarse-grained entity typing, but we show that it is outperformed by zero-shot cross-lingual transfer when applied to the similar task of fine-grained entity typing. In our study of fine-grained entity typing with the FIGER type ontology for German, we show that annotation projection amplifies the English model’s tendency to underpredict level 2 labels and is beaten by zero-shot cross-lingual transfer on three novel test sets.

#### Introduction

Annotation projection has been used successfully for the task of coarse-grained named entity typing in conjunction with named entity recognition (Agerri et al., 2018; Li et al., 2021; Ni et al., 2017). We follow these examples by using a parallel English-German corpus, automatic named entity recognition and a state of the art English FET model [[2020 Chen - Hierarchical Entity Typing via Multi-level Learning to Rank |Chen et al 2020]] to assign FIGER type labels on the English side for transfer. We then project the labels onto the German half of the corpus. The output of this process is a German corpus annotated with FIGER types, which we use to train a German FET model.

Another approach to the same problem is zero-shot cross-lingual transfer, in which a model built on multilingual word embeddings and trained on high-resource language data is applied to test data in a different language. Because the English FET model used in this work [[2020 Chen - Hierarchical Entity Typing via Multi-level Learning to Rank |Chen et al 2020]] relies on contextualised multilingual word embeddings (XLM-RoBERTa) (Conneau et al., 2019) it is possible to train it on English data and to test it on German.

We compare the two approaches and show that the annotation projection approach amplifies the model’s tendency to underpredict level 2 types, which lowers model performance. We also introduce three new test sets for German FET on which zero-shot cross-lingual transfer performs better than models trained with German or a mix of German and English data.

#### Method
In this work **we use the hierarchical typing model of Chen et al. (2020) trained on English gold data for the zero-shot approach and also to annotate the English side of the parallel text for annotation projection**. We train the model with English silver data to show the amount of noise added by automatic annotation and finally we train it with German data which was produced by annotation projection.

In the hierarchical typing model the entity and its context are encoded using multilingual XLMRoBERTa (Conneau et al., 2019). For each type in the FIGER ontology the model learns a type embedding. It passes the concatenated entity and context vector through a 2-layer feed-forward network that maps into the same space as the type embedding. The score is the inner product between the transformed entity and context vector and the type embedding. For further model details refer to Chen et al. (2020).

#### Experimental Setup

**Training Data** 

To contrast the zero-shot crosslingual transfer approach with models trained on automatically annotated and projected data we use three sources of training data. We use the 2M sentences English FIGER corpus as described by (Ling and Weld, 2012) as a source of **English human annotated data, which we will refer to as EN gold**. The data set consists of English Wikipedia articles and we use it to train the zero-shot gold model.

Second, we use **English machine annotated data (EN automatic)**. Annotating English data using a model is the first step of the annotation projection. We use this data to train the zero-shot automatic model to examine the amount of noise added by automatic annotation. We generate EN automatic from English sentences from the WikiMatrix corpus Schwenk et al. (2019), using the hierarchical typing model trained on 2M sentences EN gold

Lastly, we use **German annotation projected data (DE projected)** that was generated by projecting the labels from the EN automatic onto German. For the details of our annotation projection pipeline please refer to appendix A. We use this data to train the annotation projected model

We portion each training corpus into slices of 100, 200, 300 and 400 K sentences to compare the influence of data size. For DE projected only 300 K sentences are available, because only part of the parallel sentences in the WikiMatrix corpus are of high enough quality for annotation projection. For details of the selection process refer to appendix A.

An important point for our experiments is the label distribution in the training corpora (see table 1). **The hierarchical typing model has the tendency to underpredict the finer-grained level 2 labels** (e.g. /person/actor, as opposed to level 1 label /person), which leads to a different distribution of labels in EN gold and the other corpora. Compared to approximately 100 K level 2 labels per 100 K sentences in the gold data, we only see about 50 K level 2 labels in the silver data. This tendency does not depend on the different input data: If we use a model trained on 100 K EN gold to predict labels on an unseen portion of EN gold, only 25% of the resulting annotations are level 2 labels.

#### Discussion

![[2021_Weber_Performance.png]]

Figure 2 compares the performance of the models *zero-shot gold, zero-shot automatic* and *annotation projected* at different training data sizes on the DE-FIGER and the DE-Wiki test sets. *Zero-shot gold* outperforms *zero-shot automatic* and *annotation projected* on both test sets and in all training data sizes. *Zero-shot gold* trained on the full EN gold data set of 2 M sentences performs only 1% better on level 1 labels and 3% better on level 2 labels than a model trained with 400 K sentences, which shows that smaller data slices are sufficient to reach most of the possible performance with this data set.

While for level 1 type labels annotation projected gets close to the performance of zero-shot gold on both test sets, on level 2 type labels the system falls behind zero-shot gold, with a wider gap on DE-Wiki. The comparison between zero-shot automatic and annotation projected is less clear. On the DE-Wiki test set annotation projected consistently outperforms zero-shot automatic, while on DE-FIGER both systems perform very similarly. The high performance of zero-shot gold and the noisier zero-shot automatic might be due to the quality of English and German embeddings in XLM-RoBERTa, as both are high resource languages from the same language family. This confirms Lauscher et al. (2020) who show that this method works especially well for close high resource language pairs and low level semantic tasks.

The noise introduced by annotation projection affects level 2 label performance the most (see appendix B and table 1). But the amount of level 2 labels in the training data can not be the only reason for this. The total number of labels in the silver corpora (see table 1) shows that 200 K of silver training data contain approximately the same amount of level 2 labels as 100 K of gold data. Nevertheless, the level 2 performance of systems trained on 200 K of silver data lies behind the model trained on 100 K of EN gold. This points towards the possibility, that not only the amount of level 2 labels in the training data, but also their quality and their proportion with level 1 labels play a role here.

**Multilingual training** 

![[2021_weber_table_3.png]]

The underlying XLMRoBERTa embeddings allow to train a model with both German and English data. For this we combine slices from DE projected with EN automatic, because these data sets have the same distribution of labels. Table 3 shows the performance of a model trained with evenly mixed data (EN+DE) in comparison with monolingually trained models of the same size tested on DE-FIGER and EN-FIGER. German performance benefits from using both German and English training data, while performance in English is best with only English data. The mixed model does not outperform zero-shot gold on these test sets.

The low performance in the data mixing scenario compared to zero-shot gold can be explained with the distribution of labels in the silver corpora. Due to the noise added when labels are projected from English to German, the mixed model tested in German profits from the addition of higher quality English data, but not vice versa.

**Few-shot training** 

Zhao et al. (2020) suggest that few-shot learning improves zero-shot performance. To test this we take a model trained on 100 K sentences EN gold and fine-tune it by training on the 135 sentence manually annotated DE-GermEnt data set. We evaluate the resulting model’s performance on DE-FIGER. In comparison with the model trained on 100 K EN gold only, the performance of the resulting model is 10% lower in accuracy of level 1 labels and 12% lower on level 2 labels. We did not specifically select which sentences to use like Zhao et al. (2020), which is an avenue for future work. The low performance of the few-shot model could be due to the high number of different labels, only a few of which can be observed during few-shot training, but further work is needed to confirm this.

**German entities** 

To challenge zero-shot gold, we test a model trained 2 M sentences EN gold on the test set DE-GermEnt. Surprisingly, we find that the model performs better on DE-GermEnt than on its English entity counter part, with 1% higher performance on level 1 labels and 3% higher performance on level 2 labels. It is unclear why zero-shot gold behaves this way, and examining this with larger challenge data sets it an avenue for future work.

#### Discussion and Conclusion

Our results show that zero-shot cross-lingual transfer building upon XLM-RoBERTa is a strong baseline for the task of FET and the language pair of English and German. It outperforms annotation projection on three new test sets. We also show that in our specific scenario annotation projection using the hierarchical typing model amplifies the models tendency to underpredict level 2 types.

One way to mitigate these shortcomings would be to sample level 1 and level 2 labels in a training corpus so that they have the same distribution as in the gold data, although this would not control for data quality. Another way could be to machine translate the manually annotated English corpus into German and then use annotation projection, as suggested by Ehrmann et al. (2011). This way the label distribution of the human annotated data could be preserved as well. Lastly, improving the few-shot approach and designing more challenging test sets are other avenues to explore.

---
Sabine Weber
Mark Steedman

University of Edinburgh 
