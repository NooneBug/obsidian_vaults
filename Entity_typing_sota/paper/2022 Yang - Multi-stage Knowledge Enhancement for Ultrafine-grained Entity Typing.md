https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757340

To inject knowledge into fine-grained entity typing, we enhance both CWRs and the typing structure. We adopt RoBERTa as our sentence encoder and design two plug-in modules, cross-attention module and entity-reasoning module, to obtain implicitly enhanced CWRs. Then a task specific module is stacked on the top to enhance entity typing explicitly. Plug-in structures are able to work with original parameters of the backbone encoder fixed, thus being faster and lighter than previous methods. For cross-attention module, we propose a contrastive objective that provides a unified way to utilize multi-source distant-labeled data. With entity-reasoning module and entity typing module, we show a scheme of organizing soft-labels with a relation matrix for entity types and reasoning with them



#paper