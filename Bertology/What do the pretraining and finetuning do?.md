Part of the [[Bertology Video]]

As showed in [[Revealing the dark secrets of BERT]], BERT works well in many tasks also without a pretraining

As showed in [[Revealing the dark secrets of BERT]], last transformers (or layers) are the most task-specific and the most affected by fine-tuning  (attention heads are analysed using the difference between their average value on the total dataset before and after the fine tuning)

As showed in [[Revealing the dark secrets of BERT]] fine-tuning does **NOT** necessarily '*teach*' informative attention patterns