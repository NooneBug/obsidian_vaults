[Talk](https://www.youtube.com/watch?v=HOIrAEXS-ck) [Slides](https://u.pcloud.link/publink/show?code=XZRYtNXZPKAWDwhNefuSIGuOnHEQ68NlySDy)

Outline
---
- [[BERT]]
- [[Attention]]
	- [[How important are attention heads?]] 
- How does BERT learns
	- [[What do the pretraining and finetuning do?]]
- [[Does lottery ticket hypotesis hold for BERT]]
- [[What does it takes to disrupt BERT]]
- [[What does BERT learn?]] From this some follow-up questions are extracted:
	- Can we learn to generalize with smaller models?
	- Can we pretrain transformers more efficently, given the outlier phenomenon and attention head redundancy?
	- Can we get the Transformers to learn more high-level and/or interpretable linguistic/logical patterns for NLU tasks? 
	- Can we tell when BERT is right for the right reasons?